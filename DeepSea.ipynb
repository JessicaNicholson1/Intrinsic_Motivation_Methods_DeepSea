{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e61dbcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessicanicholson/jupy/jup_notebook/lib/python3.10/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import bsuite\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from scipy import stats\n",
    "from scipy.stats import norm \n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b45864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions, item = True):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state,action)\n",
    "            which returns Q(state,action)\n",
    "          - self.set_qvalue(state,action,value)\n",
    "            which sets Q(state,action) := value\n",
    "        !!!Important!!!\n",
    "        Note: please avoid using self._qValues directly. \n",
    "            There's a special self.get_qvalue/set_qvalue for that.\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.item = item\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        Compute your agent's estimate of V(s) using current q-values\n",
    "        V(s) = max_over_action Q(state,action) over possible actions.\n",
    "        Note: please take into account that q-values can be negative.\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        value = max([self.get_qvalue(state, a) for a in possible_actions])\n",
    "        return value\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here:\n",
    "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "        \n",
    "        q = reward + gamma * (1 - done) * self.get_value(next_state)\n",
    "        q = (1 - learning_rate) * self.get_qvalue(state, action) + learning_rate * q\n",
    "        \n",
    "        if self.item:\n",
    "            self.set_qvalue(state, action, q.item())\n",
    "        else:\n",
    "            self.set_qvalue(state, action, q)\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values). \n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "                \n",
    "        idx = np.argmax([self.get_qvalue(state, a) for a in possible_actions])\n",
    "\n",
    "        return possible_actions[idx]\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.  \n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list). \n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        action = None\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(possible_actions)\n",
    "        \n",
    "        return self.get_best_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a374bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        \n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        \n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward.item)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)            \n",
    "        return (\n",
    "            np.array(obses_t),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(obses_tp1),\n",
    "            np.array(dones)\n",
    "        )\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [ random.randint(0, len(self._storage) - 1)\n",
    "            for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "919e3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_number(s):\n",
    "    return np.argmax(s.flatten())\n",
    "\n",
    "def test_agent(agent, greedy=True, delay=.5):\n",
    "    v = get_all_states_value(agent)\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        fig, ax = plt.subplots(ncols=2)\n",
    "        ax[0].imshow(s)\n",
    "        ax[0].set_title('State')\n",
    "        im = ax[1].imshow(v)\n",
    "        plt.colorbar(im)\n",
    "        ax[1].set_title('Value function')\n",
    "        clear_output(True)\n",
    "        plt.show()\n",
    "        s = get_state_number(s)\n",
    "        \n",
    "        if greedy:\n",
    "            a = agent.get_best_action(s)\n",
    "        else:\n",
    "            a = agent.get_action(s)\n",
    "\n",
    "        s, r, terminated, truncated, _  = env.step(a)\n",
    "        done = terminated or truncated\n",
    "        sleep(delay)\n",
    "\n",
    "def get_all_states_value(agent):\n",
    "    s_shape = env.observation_space.shape\n",
    "    s_shape_flatten = np.prod(s_shape)\n",
    "    v = np.zeros(s_shape_flatten)\n",
    "    \n",
    "    for i in range(s_shape_flatten):\n",
    "        v[i] = agent.get_value(i)\n",
    "    v = v.reshape(s_shape)\n",
    "    return v\n",
    "\n",
    "def to_one_hot(x, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    x = x.long().view(-1, 1)\n",
    "    x = torch.zeros(x.size()[0], ndims).scatter_(1, x, 1)\n",
    "    return x\n",
    "\n",
    "def freeze_test_agent(agent, test_episodes):\n",
    "    \n",
    "    all_rewards = []\n",
    "    for i in range(test_episodes):\n",
    "        eps_rewards = 0\n",
    "        v    = get_all_states_value(agent)\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            s = get_state_number(s)\n",
    "            a = agent.get_best_action(s)\n",
    "            n_s, r, terminated, truncated, _  = env.step(a)\n",
    "            eps_rewards+=r\n",
    "            done = terminated or truncated\n",
    "            s = n_s\n",
    "        all_rewards.append(eps_rewards)\n",
    "    \n",
    "    return all_rewards\n",
    "\n",
    "def average_first_visits(episode):\n",
    "    \n",
    "    s_shape = env.observation_space.shape\n",
    "    s_shape_flatten = np.prod(s_shape)\n",
    "    available_states = {i: None for i in range(s_shape_flatten)}\n",
    "\n",
    "    states = [np.argmax(state) for state in episode]\n",
    "    states\n",
    "    first_visits = {}\n",
    "    for timestep, state in enumerate(states):\n",
    "        if state not in first_visits:\n",
    "            first_visits[state] = timestep\n",
    "\n",
    "    not_visited_states = [state for state in available_states if state not in first_visits]\n",
    "\n",
    "    num_steps = env.observation_space.shape[0]\n",
    "    for state in not_visited_states:\n",
    "            first_visits[state] = 1000  \n",
    "\n",
    "    new_sea = np.zeros(s_shape_flatten)\n",
    "\n",
    "    for state, first_visit in first_visits.items():\n",
    "        new_sea[state] = first_visit\n",
    "    \n",
    "    return new_sea\n",
    "\n",
    "def visualize_single_run(episode, agent_states):\n",
    "    \n",
    "    s_shape = env.observation_space.shape\n",
    "\n",
    "    for states in range(episode):\n",
    "        new_sea = average_first_visits(agent_states[states])\n",
    "\n",
    "        if states == 0:\n",
    "            seas = new_sea\n",
    "        else:\n",
    "            seas = np.vstack([seas, new_sea])\n",
    "\n",
    "    average_visits = np.mean(seas, axis = 0)        \n",
    "    \n",
    "    return average_visits\n",
    "\n",
    "    plt.imshow(average_visits.reshape(s_shape), cmap='Spectral')\n",
    "    plt.title(f'Average First-visit visualizations\\n{episode} Episodes')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def visualize_multiple_runs(all_episodes, epsilon, agent_states, learning_agent, num_episodes):\n",
    "    s_shape = env.observation_space.shape\n",
    "    all_average_visits = []\n",
    "\n",
    "    for i, episode in enumerate(all_episodes):\n",
    "        seas = None \n",
    "        for states in range(episode):\n",
    "            new_sea = average_first_visits(agent_states[states])\n",
    "\n",
    "            if states == 0:\n",
    "                seas = new_sea\n",
    "            else:\n",
    "                seas = np.vstack([seas, new_sea])\n",
    "\n",
    "        average_visits = np.mean(seas, axis=0)\n",
    "        all_average_visits.append(average_visits)\n",
    "\n",
    "    print(f'average_first_visits {learning_agent} Agent for {num_episodes} Episodes')\n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    _, axarr1 = plt.subplots(1,len(all_episodes), figsize = (15,15))\n",
    "    axarr1[0].set_ylabel(f'Epsilon: {epsilon}')\n",
    "\n",
    "    for i in range(len(all_average_visits)):\n",
    "\n",
    "        axarr1[i].imshow(all_average_visits[i].reshape(s_shape), cmap='Spectral')\n",
    "\n",
    "        axarr1[i].set_xticks([])\n",
    "        axarr1[i].set_yticks([])\n",
    "        axarr1[i].set_title(f'{all_episodes[i]} Episodes')\n",
    "\n",
    "        if i > len(all_episodes)-2:\n",
    "            last_subplot = axarr1[-1]\n",
    "            cax = last_subplot.imshow(all_average_visits[i].reshape(s_shape), cmap='RdYlBu') \n",
    "            plt.colorbar(cax, ax=last_subplot,  fraction=0.05)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def average_test_rewards(rewards):\n",
    "    \n",
    "    all_avgs = []\n",
    "\n",
    "    for row in zip(rewards[0], rewards[1], rewards[2]):\n",
    "\n",
    "        new_rows = []\n",
    "        for i in zip(row[0], row[1], row[2]):\n",
    "            m = np.mean(i)\n",
    "            new_rows.append(m)\n",
    "        all_avgs.append(new_rows)\n",
    "    all_avgs\n",
    "    \n",
    "    return all_avgs\n",
    "\n",
    "def flatten_rewards(test_rewards):\n",
    "    flat_test_rewards = []\n",
    "    for sublist in test_rewards:\n",
    "        flat_test_rewards.extend(sublist)\n",
    "    return flat_test_rewards\n",
    "    \n",
    "    \n",
    "def state_visitation_percentage(env, episodes, states, learning_agent):\n",
    "    # Count all available states in env \n",
    "    all_states = np.prod(env.observation_space.shape)\n",
    "    diagonal = env.observation_space.shape[0]/2\n",
    "    available_states = all_states/2 + diagonal\n",
    "\n",
    "    # Get full list of visited states\n",
    "    flat_states = []\n",
    "    for sublist in states:\n",
    "        flat_states.extend(sublist)\n",
    "\n",
    "    # Get location of visited states    \n",
    "    state_loc = []\n",
    "    for i in flat_states:\n",
    "        state = np.argmax(i)\n",
    "        if state != 0:\n",
    "            state_loc.append(state)\n",
    "            \n",
    "    # Count the unique visited states\n",
    "    visited_states = set(state_loc)\n",
    "    num_visited_states = len(visited_states)\n",
    "    \n",
    "    # Calculate the percentage of visited states\n",
    "    percentage_visited = (num_visited_states / available_states) * 100\n",
    "\n",
    "    print(f\"% of total states visited/ {episodes} Eps: {percentage_visited:.2f}%  {learning_agent} \")\n",
    "    \n",
    "    return percentage_visited\n",
    "\n",
    "def get_state_visitation_percentage(env, states):\n",
    "    # Count all available states in env \n",
    "    all_states = np.prod(env.observation_space.shape)\n",
    "    diagonal = env.observation_space.shape[0]/2\n",
    "    available_states = all_states/2 + diagonal\n",
    "\n",
    "    # Get full list of visited states\n",
    "    flat_states = []\n",
    "    for sublist in states:\n",
    "        flat_states.extend(sublist)\n",
    "\n",
    "    # Get location of visited states    \n",
    "    state_loc = []\n",
    "    for i in flat_states:\n",
    "        state = np.argmax(i)\n",
    "        if state != 0:\n",
    "            state_loc.append(state)\n",
    "            \n",
    "    # Count the unique visited states\n",
    "    visited_states = set(state_loc)\n",
    "    num_visited_states = len(visited_states)\n",
    "    \n",
    "    # Calculate the percentage of visited states\n",
    "    percentage_visited = (num_visited_states / available_states) * 100\n",
    "    \n",
    "    return percentage_visited\n",
    "\n",
    "\n",
    "def plot_ftv_histograms(states):\n",
    "    s_shape = env.observation_space.shape\n",
    "    s_shape_flatten = np.prod(s_shape)\n",
    "    available_states = {i: 0 for i in range(s_shape_flatten)}\n",
    "\n",
    "    all_flat_states = []\n",
    "    for agent_states in states:\n",
    "        flat_states = []\n",
    "        for sublist in agent_states:\n",
    "            flat_states.extend(sublist)\n",
    "        all_flat_states.append(flat_states)\n",
    "\n",
    "    merged_dicts = []\n",
    "    for run in range(len(all_flat_states)):\n",
    "\n",
    "        first_visits = {}\n",
    "        for timestep, state in enumerate(all_flat_states[run]):\n",
    "            if state not in first_visits:\n",
    "                first_visits[state] = timestep\n",
    "\n",
    "        merged_dict = {key: first_visits[key] if key in first_visits else available_states[key] for key in available_states}\n",
    "        merged_dicts.append(merged_dict)\n",
    "\n",
    "    arr = np.array([list(d.values()) for d in merged_dicts])\n",
    "\n",
    "    mean_values = np.mean(arr, axis=0)\n",
    "\n",
    "    mean_dict = {key: int(mean_values[i]) for i, key in enumerate(merged_dicts[0].keys())}\n",
    "\n",
    "    values = list(mean_dict.values())\n",
    "\n",
    "    grid_size = env.observation_space.shape[0]\n",
    "    state_values = [[i + j*grid_size + 1 for i in range(grid_size)] for j in range(grid_size)]\n",
    "    lower_diagonal_values = []\n",
    "\n",
    "    for row in range(grid_size):\n",
    "        for col in range(grid_size):\n",
    "            if row >= col:\n",
    "                lower_diagonal_values.append(state_values[row][col] - 1)  # Subtract 1 here\n",
    "\n",
    "    updated_dict = {state: mean_dict[state] for state in lower_diagonal_values}\n",
    "\n",
    "    for key, val in updated_dict.items():\n",
    "        if val == 0:\n",
    "            updated_dict[key] = 50000\n",
    "\n",
    "    return updated_dict.values()\n",
    "\n",
    "\n",
    "def plot_histograms(all_is, label, color):\n",
    "    data = list(plot_ftv_histograms(all_is))\n",
    "    mu, std = norm.fit(data)\n",
    "    sns.histplot(data, bins=20, kde=True, color = color)  \n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    plt.plot(x, p, 'k', linewidth=2, label = label, color=color)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee11ee0",
   "metadata": {},
   "source": [
    "# Epsilon-Annealing Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_annealing(env, agent, seed, num_episodes, test_reward_period, initial_epsilon, final_epsilon):\n",
    "    all_states  = []\n",
    "    all_test_rewards = []\n",
    "    all_percentages = []\n",
    "    \n",
    "    test_n = 0\n",
    "    target_value = 0.99\n",
    "\n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        s, _ = env.reset(seed=seed)\n",
    "        done = False\n",
    "        episode_rewards = 0\n",
    "        episode_states  = []\n",
    "        episode_is = []\n",
    "        epsilon = max(final_epsilon, initial_epsilon * (1 - i / num_episodes))\n",
    "\n",
    "        while not done:\n",
    "            episode_states.append(s)\n",
    "            i_s = get_state_number(s)\n",
    "            episode_is.append(i_s)\n",
    "            \n",
    "            if np.random.rand() < epsilon:\n",
    "                a = env.action_space.sample()  # Exploration\n",
    "            else:\n",
    "                i_s = get_state_number(s)\n",
    "                a = agent.get_action(i_s)  # Exploitation\n",
    "\n",
    "            s_next, r, terminated, truncated, _ = env.step(a)\n",
    "            episode_rewards += r\n",
    "            done = terminated or truncated\n",
    "            i_s_next = get_state_number(s_next)\n",
    "            agent.update(i_s, a, r, i_s_next, terminated)\n",
    "            s = s_next\n",
    "\n",
    "        all_states.append(episode_states)\n",
    "        \n",
    "        if i % test_reward_period == 0:\n",
    "            test_n += 1\n",
    "            tests = freeze_test_agent(agent, 10)\n",
    "            all_test_rewards.append(np.mean(tests))\n",
    "            \n",
    "            percentages = get_state_visitation_percentage(env, all_states)\n",
    "            all_percentages.append(percentages)\n",
    "\n",
    "    return all_states, all_test_rewards, all_percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "learning_agent = 'qlearning'\n",
    "\n",
    "anneal_all_agent_percentages = []\n",
    "anneal_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    initial_epsilon = 1.0\n",
    "    final_epsilon = 0.1\n",
    "    epsilon = 0.1\n",
    "    num_episodes = 5000       \n",
    "    test_reward_period = 100\n",
    "\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = False)\n",
    "\n",
    "    anneal_states, anneal_test_rew, anneal_percentages, = epsilon_annealing(env,\n",
    "                                                                             agent,\n",
    "                                                                             seed, \n",
    "                                                                             num_episodes,\n",
    "                                                                             test_reward_period,\n",
    "                                                                             initial_epsilon,\n",
    "                                                                             final_epsilon)\n",
    "    \n",
    "    anneal_all_agent_percentages.append(anneal_percentages)\n",
    "    anneal_all_states.append(anneal_states)\n",
    "    \n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}q_learning_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, anneal_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}q_learning_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, anneal_all_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72d8e6",
   "metadata": {},
   "source": [
    "# Intrinsic Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f29883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseIntrinsicRewardModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        return 0.0\n",
    "\n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c335fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        def init_weights(tensor):\n",
    "            if isinstance(tensor, nn.Linear):\n",
    "                nn.init.xavier_uniform_(tensor.weight)\n",
    "        self.layers.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_reward(env, agent, seed, reward_module, intrinsic_weight, n_episodes, test_reward_period,\n",
    "                      update_reward_period, batch_size, n_iter, std_dev, annealing = True):\n",
    "    \n",
    "    buffer = ReplayBuffer(size=int(1e6))\n",
    "    \n",
    "    if list(reward_module.parameters()):\n",
    "        optimizer = torch.optim.Adam(reward_module.parameters())\n",
    "    else:\n",
    "        optimizer = None\n",
    "                \n",
    "    losses = []\n",
    "    \n",
    "    previous_mean = 0\n",
    "    rolling_mean_window = 50\n",
    "    rolling_means = [] \n",
    "    test_n = 0\n",
    "    target_value = 0.99\n",
    "    \n",
    "    all_test_rewards = []    \n",
    "    all_intrinsic_rewards = []\n",
    "    all_rewards  = []\n",
    "    all_states = []\n",
    "    all_percentages = []\n",
    "    \n",
    "    for i in tqdm(range(n_episodes)):\n",
    "        s, _   = env.reset(seed=seed)\n",
    "\n",
    "        done = False\n",
    "        episode_rewards = 0\n",
    "        episode_states  = []\n",
    "        episodes_int_rews = []\n",
    "        episode_is = []\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            steps+=1\n",
    "            \n",
    "            episode_states.append(s)\n",
    "            i_s = get_state_number(s)\n",
    "            episode_is.append(i_s)\n",
    "            \n",
    "            a = agent.get_action(i_s)\n",
    "            s_next, r, terminated, truncated, _  = env.step(a)\n",
    "            episode_rewards += r\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            i_s_next = get_state_number(s_next)\n",
    "            state_t  = torch.tensor(s).float().view(1, -1)        \n",
    "            action_t = torch.tensor(a).float().view(1, -1)\n",
    "            next_state_t = torch.tensor(s_next).float().view(1, -1)\n",
    "            \n",
    "            r_intr = intrinsic_weight * reward_module.get_intrinsic_reward(state_t, action_t, next_state_t)\n",
    "            episodes_int_rews.append(r_intr)\n",
    "            r += r_intr\n",
    "\n",
    "            agent.update(i_s, a, r, i_s_next, terminated)\n",
    "            buffer.add(state_t, a, r, next_state_t, terminated)\n",
    "            \n",
    "            s = s_next\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "                \n",
    "        all_rewards.append(episode_rewards)\n",
    "        all_states.append(episode_states)\n",
    "        all_intrinsic_rewards.append(np.mean(episodes_int_rews))\n",
    "        \n",
    "        \"\"\"Anneal Intrinsic Reward\"\"\"\n",
    "        if annealing:\n",
    "            if (i +1) % 50 == 0:\n",
    "                current_mean  = np.mean(all_rewards[-rolling_mean_window:])\n",
    "                if current_mean > previous_mean:\n",
    "                    intrinsic_weight *= 0.9\n",
    "                previous_mean = current_mean\n",
    "        \n",
    "        \"\"\"Freeze & Test Policy\"\"\"\n",
    "        if (i+1) % test_reward_period == 0:\n",
    "            test_n+=1\n",
    "            test_rewards = freeze_test_agent(agent,10)\n",
    "            all_test_rewards.append(np.mean(test_rewards))\n",
    "            \n",
    "            percentages = get_state_visitation_percentage(env, all_states)\n",
    "            all_percentages.append(percentages)\n",
    "\n",
    "\n",
    "        \"\"\"Update Intrinsic Module\"\"\"\n",
    "        if (i + 1) % update_reward_period == 0 and optimizer is not None:\n",
    "        \n",
    "            for _ in range(n_iter):\n",
    "                optimizer.zero_grad()\n",
    "                state_batch, action_batch, _, next_state_batch, _ = buffer.sample(batch_size)\n",
    "                state_tensor  = torch.tensor(state_batch).float().flatten(1, 2)\n",
    "\n",
    "                action_tensor = torch.tensor(action_batch).float().view(-1, 1)\n",
    "                next_state_tensor = torch.tensor(next_state_batch).float().flatten(1, 2)\n",
    "                                \n",
    "                loss = reward_module.get_loss(state_tensor, action_tensor, next_state_tensor)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "                    \n",
    "    return all_rewards, all_states, all_test_rewards, all_intrinsic_rewards, all_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a167001",
   "metadata": {},
   "source": [
    "# RND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90dfd1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class RandomNetworkDistilationModule(BaseIntrinsicRewardModule):\n",
    "    \n",
    "    def __init__(self, states_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.target_network = MLP(states_size,\n",
    "                                  hidden_size,\n",
    "                                  embedding_size)\n",
    "        \n",
    "        self.predictor_network = MLP(states_size,\n",
    "                                     hidden_size,\n",
    "                                     embedding_size)\n",
    "        self.target_network.eval()\n",
    "        \n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        with torch.no_grad():\n",
    "            target_embedding = self.target_network(next_state)\n",
    "            predictor_embedding = self.predictor_network(next_state)\n",
    "            intrinsic_reward = ((target_embedding - predictor_embedding) ** 2).sum()\n",
    "        return intrinsic_reward\n",
    "\n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_embedding  = self.target_network(next_state_batch)\n",
    "            \n",
    "        predictor_embedding = self.predictor_network(next_state_batch)\n",
    "        loss    = 0.5*((target_embedding - predictor_embedding) ** 2).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34ef04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "rnd_all_agent_percentages = []\n",
    "rnd_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    time_num_episodes = 5000      \n",
    "    test_reward_period = 100\n",
    "\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = 0.1,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = False)\n",
    "\n",
    "    rnd = RandomNetworkDistilationModule(np.prod(env.observation_space.shape), \n",
    "                                         np.prod(env.observation_space.shape), \n",
    "                                         16)\n",
    "\n",
    "    rnd_rewards, rnd_states, rnd_test_rewards, rnd_ints,  rnd_percentages = train_with_reward(env,\n",
    "                                                                                agent,\n",
    "                                                                                seed,\n",
    "                                                                                rnd,\n",
    "                                                                                intrinsic_weight = 1,\n",
    "                                                                                n_episodes=time_num_episodes,\n",
    "                                                                                test_reward_period = test_reward_period,\n",
    "                                                                                update_reward_period = 100,\n",
    "                                                                                batch_size =  150,\n",
    "                                                                                n_iter =  50,\n",
    "                                                                                std_dev = 0.0, \n",
    "                                                                                annealing = True)\n",
    "                    \n",
    "    rnd_all_agent_percentages.append(rnd_percentages)\n",
    "    rnd_all_states.append(rnd_states)\n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}rnd_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, rnd_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}rnd_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, rnd_all_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79b9158",
   "metadata": {},
   "source": [
    "# ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, states_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.module = MLP(states_size,\n",
    "                          hidden_size,\n",
    "                          embedding_size)\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.module(s)\n",
    "    \n",
    "class ForwardDynamics(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, states_size, actions_size, hidden_size, alpha=.1):\n",
    "        super().__init__()\n",
    "        self.module = MLP(actions_size + states_size,\n",
    "                          hidden_size,\n",
    "                          states_size)\n",
    "        self.alpha = alpha\n",
    "        self.mean_reward = 0\n",
    "    \n",
    "    def forward(self, s, a):\n",
    "        sa = torch.cat([s, a], dim=-1)\n",
    "        return s + self.module(sa)\n",
    "    \n",
    "    \n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):\n",
    "        predicted_next_states = self.forward(state_batch, action_batch)\n",
    "        loss = F.mse_loss(predicted_next_states, next_state_batch)\n",
    "        return loss\n",
    "    \n",
    "class InverseDynamics(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, states_size, actions_size, hidden_size, alpha=0.1):\n",
    "        super().__init__()\n",
    "        self.module = MLP(2 * states_size,\n",
    "                          hidden_size,\n",
    "                          actions_size)\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.mean_reward = 0\n",
    "        self.actions_size = actions_size\n",
    "    \n",
    "    def forward(self, s, s_next):\n",
    "        combined_state = torch.cat((s, s_next), dim=-1)\n",
    "        a_pred_proba = self.module(combined_state)\n",
    "        return torch.softmax(a_pred_proba, dim=-1)    \n",
    "\n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):        \n",
    "        a_pred_proba = self.forward(state_batch, next_state_batch)\n",
    "        a_one_hot    = to_one_hot(action_batch, self.actions_size)\n",
    "        return -(torch.log(a_pred_proba) * a_one_hot).sum(dim=-1).mean()\n",
    "\n",
    "    \n",
    "class ICMModule(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, states_size, actions_size, hidden_size, embedding_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.forward_model = ForwardDynamics(embedding_size,\n",
    "                                             actions_size = 1, \n",
    "                                             hidden_size = hidden_size)\n",
    "        self.inverse_model = InverseDynamics(embedding_size,\n",
    "                                             actions_size, \n",
    "                                             hidden_size, \n",
    "                                             alpha=0.1)\n",
    "        self.embedder = Embedder(states_size, \n",
    "                                 embedding_size, \n",
    "                                 hidden_size)\n",
    "        self.n = 1\n",
    "        self.mean_reward = 0\n",
    "        self.actions_size = actions_size\n",
    "        \n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        with torch.no_grad():       \n",
    "            phi_hat_s      = self.embedder.forward(state)\n",
    "            phi_hat_next_s = self.embedder.forward(next_state)\n",
    "            phi_pred_next_state = self.forward_model.forward(phi_hat_s, action)\n",
    "            intrinsic_reward = 0.3*(phi_pred_next_state - phi_hat_next_s).pow(2).mean()\n",
    "        return intrinsic_reward\n",
    "    \n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):\n",
    "        phi_s_batch      = self.embedder(state_batch)\n",
    "        phi_next_s_batch = self.embedder(next_state_batch)\n",
    "        forward_loss   = self.forward_model.get_loss(phi_s_batch.detach(), action_batch, phi_next_s_batch.detach())\n",
    "        inverse_loss   = self.inverse_model.get_loss(phi_s_batch, action_batch, phi_next_s_batch)\n",
    "        intrinsic_loss = forward_loss + inverse_loss\n",
    "        \n",
    "        return intrinsic_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee7c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "icm_all_agent_percentages = []\n",
    "icm_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    time_num_episodes = 5000       \n",
    "    test_reward_period = 100\n",
    "\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = 0.1,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = False)\n",
    "\n",
    "    icm = ICMModule(states_size = np.prod(env.observation_space.shape), \n",
    "                    actions_size   = env.action_space.n, \n",
    "                    hidden_size = 16,\n",
    "                    embedding_size = 32)\n",
    "\n",
    "    _, icm_states, icm_test_rewards, icm_ints, icm_percentages = train_with_reward(env,\n",
    "                                                                        agent,\n",
    "                                                                        seed, \n",
    "                                                                        icm,\n",
    "                                                                        intrinsic_weight = 1,\n",
    "                                                                        n_episodes=time_num_episodes,\n",
    "                                                                        test_reward_period = test_reward_period,\n",
    "                                                                        update_reward_period = 100,\n",
    "                                                                        batch_size =  150,\n",
    "                                                                        n_iter =  50,\n",
    "                                                                        std_dev = 0.0, \n",
    "                                                                        annealing = True)\n",
    "    icm_all_agent_percentages.append(icm_percentages)\n",
    "    icm_all_states.append(icm_states)\n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}icm_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, icm_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}icm_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, icm_all_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b375ff",
   "metadata": {},
   "source": [
    "# Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef84b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountBasedModule(BaseIntrinsicRewardModule):\n",
    "    \n",
    "    def __init__(self, hash_functions, states_size, beta):\n",
    "        super().__init__()\n",
    "        self.hash = {}\n",
    "        self.A = np.random.normal(0, 1, (hash_functions, states_size))\n",
    "        self.beta = beta\n",
    "        \n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        counts = []\n",
    "        state = state.cpu().numpy()[0]\n",
    "        hash_values = np.dot(self.A, state)\n",
    "        key  = str(np.sign(hash_values))\n",
    "        \n",
    "        if key in self.hash:\n",
    "                self.hash[key] += 1\n",
    "        else:\n",
    "            self.hash[key] = 1\n",
    "\n",
    "        counts.append(self.hash[key])    \n",
    "        \n",
    "        intrinsic_reward = beta / np.sqrt(counts[0])\n",
    "        \n",
    "        return torch.tensor(intrinsic_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee6a4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "count_all_agent_percentages = []\n",
    "count_all_states = []\n",
    "    \n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    time_num_episodes = 5000     \n",
    "    test_reward_period = 100\n",
    "    hash_functions = 32\n",
    "    beta = 1\n",
    "    states_size = np.prod(env.observation_space.shape)\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = True)\n",
    "\n",
    "    count = CountBasedModule(hash_functions, states_size, beta)\n",
    "    \n",
    "    count_rewards, count_states, count_test_rewards, count_ints, count_percentages = train_with_reward(env,\n",
    "                                                                        agent,\n",
    "                                                                        seed,\n",
    "                                                                        count,\n",
    "                                                                        intrinsic_weight = 1,\n",
    "                                                                        n_episodes=time_num_episodes,\n",
    "                                                                        test_reward_period = test_reward_period,\n",
    "                                                                        update_reward_period = 100,\n",
    "                                                                        batch_size =  150,\n",
    "                                                                        n_iter =  50,\n",
    "                                                                        std_dev = 0.0,\n",
    "                                                                        annealing = True)\n",
    "    count_all_agent_percentages.append(count_percentages)\n",
    "    count_all_states.append(count_states)\n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}count_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, count_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}count_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, count_all_states[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326448f3",
   "metadata": {},
   "source": [
    "# RIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d46a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, states_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.module = MLP(states_size,\n",
    "                          hidden_size,\n",
    "                          embedding_size)\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.module(s)\n",
    "    \n",
    "class ForwardDynamics(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, states_size, actions_size, hidden_size, alpha=.1):\n",
    "        super().__init__()\n",
    "        self.module = MLP(actions_size + states_size,\n",
    "                          hidden_size,\n",
    "                          states_size)\n",
    "        self.alpha = alpha\n",
    "        self.mean_reward = 0\n",
    "    \n",
    "    def forward(self, s, a):\n",
    "        sa = torch.cat([s, a], dim=-1)\n",
    "        return s + self.module(sa)\n",
    "    \n",
    "    \n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):\n",
    "        predicted_next_states = self.forward(state_batch, action_batch)\n",
    "        loss = F.mse_loss(predicted_next_states, next_state_batch)\n",
    "        return loss\n",
    "    \n",
    "class InverseDynamics(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, states_size, actions_size, hidden_size, alpha=0.1):\n",
    "        super().__init__()\n",
    "        self.module = MLP(2 * states_size,\n",
    "                          hidden_size,\n",
    "                          actions_size)\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.mean_reward = 0\n",
    "        self.actions_size = actions_size\n",
    "    \n",
    "    def forward(self, s, s_next):\n",
    "        combined_state = torch.cat((s, s_next), dim=-1)\n",
    "        a_pred_proba = self.module(combined_state)\n",
    "        return torch.softmax(a_pred_proba, dim=-1)    \n",
    "\n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):        \n",
    "        a_pred_proba = self.forward(state_batch, next_state_batch)\n",
    "        a_one_hot    = to_one_hot(action_batch, self.actions_size)\n",
    "        return -(torch.log(a_pred_proba) * a_one_hot).sum(dim=-1).mean()\n",
    "\n",
    "    \n",
    "class RIDEModule(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, states_size, actions_size, hidden_size, embedding_size, hash_functions, beta):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.forward_model = ForwardDynamics(embedding_size,\n",
    "                                             actions_size = 1, \n",
    "                                             hidden_size = hidden_size)\n",
    "        self.inverse_model = InverseDynamics(embedding_size,\n",
    "                                             actions_size, \n",
    "                                             hidden_size, \n",
    "                                             alpha=0.1)\n",
    "        self.embedder = Embedder(states_size, \n",
    "                                 embedding_size, \n",
    "                                 hidden_size)\n",
    "        self.count_model =CountBasedModule(hash_functions, \n",
    "                                           states_size, \n",
    "                                           beta)\n",
    "\n",
    "        self.n = 1\n",
    "        self.mean_reward = 0\n",
    "        self.actions_size = actions_size\n",
    "        \n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        \n",
    "        count_rewards = self.count_model.get_intrinsic_reward(state, action, next_state)\n",
    "\n",
    "        with torch.no_grad():       \n",
    "            state_emb      = self.embedder.forward(state)\n",
    "            next_state_emb = self.embedder.forward(next_state)\n",
    "            \n",
    "            control_rewards  = (next_state_emb - state_emb).pow(2).mean()\n",
    "            intrinsic_reward = count_rewards*control_rewards\n",
    "\n",
    "        return intrinsic_reward\n",
    "    \n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):\n",
    "        phi_s_batch      = self.embedder(state_batch)\n",
    "        phi_next_s_batch = self.embedder(next_state_batch)\n",
    "        forward_loss   = self.forward_model.get_loss(phi_s_batch.detach(), action_batch, phi_next_s_batch.detach())\n",
    "        inverse_loss   = self.inverse_model.get_loss(phi_s_batch, action_batch, phi_next_s_batch)\n",
    "        intrinsic_loss = forward_loss + inverse_loss\n",
    "        \n",
    "        return intrinsic_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "ride_all_agent_percentages = []\n",
    "ride_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    time_num_episodes = 5000        \n",
    "    test_reward_period = 100\n",
    "    hash_functions = 32\n",
    "    beta = 1\n",
    "    states_size = np.prod(env.observation_space.shape)\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = True)\n",
    "\n",
    "    ride = RIDEModule(states_size = np.prod(env.observation_space.shape), \n",
    "                      actions_size   = env.action_space.n, \n",
    "                      hidden_size = 16,\n",
    "                      embedding_size = 32,\n",
    "                      hash_functions = 32, \n",
    "                      beta = 1)\n",
    "    \n",
    "    _, ride_states, ride_test_rewards, ride_ints, ride_percentages  = train_with_reward(env,\n",
    "                                                                        agent,\n",
    "                                                                        seed,\n",
    "                                                                        ride,\n",
    "                                                                        intrinsic_weight = 1,\n",
    "                                                                        n_episodes=time_num_episodes,\n",
    "                                                                        test_reward_period = test_reward_period,\n",
    "                                                                        update_reward_period = 100,\n",
    "                                                                        batch_size =  150,\n",
    "                                                                        n_iter =  50,\n",
    "                                                                        std_dev = 0.0,\n",
    "                                                                        annealing = True)\n",
    "    ride_all_agent_percentages.append(ride_percentages)\n",
    "    ride_all_states.append(ride_states)\n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}ride_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, ride_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}ride_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, ride_all_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bca375",
   "metadata": {},
   "source": [
    "# LBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalLinear(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, reparam_noise=1e-4):\n",
    "        super(VariationalLinear, self).__init__()\n",
    "        self.mu    = nn.Linear(num_inputs, num_outputs)\n",
    "        self.sigma = nn.Linear(num_inputs, num_outputs)\n",
    "        self.reparam_noise = reparam_noise\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.mu(x)\n",
    "        sigma = self.sigma(x)\n",
    "        sigma = F.softplus(sigma) + self.reparam_noise\n",
    "        return mu, sigma\n",
    "\n",
    "class LBS_Reward(BaseIntrinsicRewardModule):\n",
    "    def __init__(self,action_size, state_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.trans_det = nn.Sequential(\n",
    "                nn.Linear(state_size + action_size, hidden_size),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, hidden_size),                \n",
    "                nn.ReLU())\n",
    "        \n",
    "        self.trans_stoc = VariationalLinear(hidden_size, state_size)  \n",
    "        self.repr_model = nn.Sequential(\n",
    "                    nn.Linear(hidden_size + state_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                    VariationalLinear(hidden_size, state_size))\n",
    "        \n",
    "        self.beta = 1\n",
    "        self.head = nn.Linear(state_size, hidden_size) \n",
    "        \n",
    "        \n",
    "    def forward(self, state, action, next_state):\n",
    "        \n",
    "        # prior\n",
    "        s_a_combined = torch.cat([state, action], dim=-1).float()\n",
    "        trans_det_states = self.trans_det(s_a_combined)\n",
    "        \n",
    "        trans_stoch_mu, trans_stoch_sigma = self.trans_stoc(trans_det_states)\n",
    "        trans_stoch_distr = D.independent.Independent(D.Normal(trans_stoch_mu, trans_stoch_sigma), 1)\n",
    "\n",
    "        # posterior \n",
    "        ns_a_combined = torch.cat([trans_det_states, next_state], dim=-1) \n",
    "        \n",
    "        repr_stoch_mu, repr_stoch_sigma = self.repr_model(ns_a_combined)\n",
    "        repr_stoch_distr = D.independent.Independent(D.Normal(repr_stoch_mu, repr_stoch_sigma), 1)\n",
    "\n",
    "        return trans_det_states, trans_stoch_distr, repr_stoch_distr\n",
    "    \n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "\n",
    "        state = torch.flatten(state, start_dim=0) \n",
    "        action = torch.tensor(action).squeeze(0) \n",
    "        next_state = torch.flatten(next_state, start_dim=0) \n",
    "\n",
    "        _, trans_pred_distr, repr_pred_distr = self.forward(state, action, next_state)\n",
    "        \n",
    "        intrinsic_reward = D.kl.kl_divergence(repr_pred_distr, trans_pred_distr) # .mean(-1)\n",
    "        \n",
    "        return intrinsic_reward.detach().numpy()\n",
    "\n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):        \n",
    "    \n",
    "        target_next_states, trans_pred_distr, repr_pred_distr = self.forward(state_batch, action_batch, next_state_batch)\n",
    "        repr_samples = repr_pred_distr.rsample()\n",
    "\n",
    "        target_distr = D.independent.Independent(D.Normal(target_next_states, torch.ones_like(target_next_states)), 1)\n",
    "        \n",
    "        repr_projections = self.head(repr_samples)\n",
    "        logprob_target = target_distr.log_prob(repr_projections)\n",
    "        \n",
    "        kl_div_post_prior = D.kl.kl_divergence(repr_pred_distr, trans_pred_distr) \n",
    "\n",
    "        loss = (self.beta * kl_div_post_prior - logprob_target).mean()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430fbfb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.distributions as D\n",
    "\n",
    "num_agents = 3\n",
    "lbs_all_agent_percentages = []\n",
    "lbs_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    time_num_episodes = 5000        \n",
    "    test_reward_period = 100\n",
    "    states_size = np.prod(env.observation_space.shape)\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = True)\n",
    "\n",
    "    lbs = LBS_Reward(action_size = 1, \n",
    "                     state_size  = np.prod(env.observation_space.shape),\n",
    "                     hidden_size = 16)\n",
    "    \n",
    "\n",
    "    lbs_rewards, lbs_states, lbs_test_rewards, lbs_ints, lbs_percentages = train_with_reward(env,\n",
    "                                                                        agent,\n",
    "                                                                        seed,\n",
    "                                                                        lbs,\n",
    "                                                                        intrinsic_weight = 1,\n",
    "                                                                        n_episodes=time_num_episodes,\n",
    "                                                                        test_reward_period = test_reward_period,\n",
    "                                                                        update_reward_period = 100,\n",
    "                                                                        batch_size =  150,\n",
    "                                                                        n_iter =  50,\n",
    "                                                                        std_dev = 0.0, \n",
    "                                                                        annealing = True)\n",
    "    lbs_all_agent_percentages.append(lbs_percentages)\n",
    "    lbs_all_states.append(lbs_states)\n",
    "    \n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}lbs_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, lbs_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}lbs_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, lbs_all_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abbb8a9",
   "metadata": {},
   "source": [
    "# Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44861024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, states_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.module = MLP(states_size,\n",
    "                          embedding_size,\n",
    "                          hidden_size)\n",
    "    def forward(self, s):\n",
    "        return self.module(s)\n",
    "\n",
    "class GaussianForwardDynamics(nn.Module):\n",
    "    def __init__(self, encoding_dim, action_size, latent_dim):\n",
    "        super().__init__()\n",
    "     \n",
    "        self.fc = nn.Linear(encoding_dim + action_size, encoding_dim)\n",
    "        self.fc_mu = nn.Linear(encoding_dim, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(encoding_dim, latent_dim)\n",
    "\n",
    "    def forward(self, latent_state, action):\n",
    "        \n",
    "        x  = torch.cat([latent_state, action], dim=-1)    \n",
    "        x  = F.gelu(self.fc(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_log_var(x)\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class SurprisalModule(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, input_size, action_size, linear_size, hidden_size, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedder = Embedder(input_size, \n",
    "                                 linear_size, \n",
    "                                 hidden_size)\n",
    "        \n",
    "        self.forward_model = GaussianForwardDynamics(hidden_size, \n",
    "                                                     1,\n",
    "                                                     hidden_size)\n",
    "        \n",
    "        self.n = 1\n",
    "        self.eta0 = 0\n",
    "\n",
    "    def get_loss_vec(self, state_batch, action_batch, next_state_batch):\n",
    "\n",
    "        phi_s_batch = self.embedder(state_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            phi_next_s_batch = self.embedder(next_state_batch)\n",
    "        \n",
    "        mu, log_var = self.forward_model.forward(phi_s_batch, action_batch)\n",
    "        dist = torch.distributions.MultivariateNormal(mu, torch.diag_embed(torch.exp(log_var)))\n",
    "        loss = -dist.log_prob(phi_next_s_batch)\n",
    "        return loss\n",
    "\n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        with torch.no_grad():\n",
    "            loss_vec = self.get_loss_vec(state, action, next_state)\n",
    "            intrinsic_reward = self.normalise_reward(loss_vec)\n",
    "        return intrinsic_reward[0] \n",
    "    \n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch, *args, **kwargs):\n",
    "        loss_vec = self.get_loss_vec(state_batch, action_batch, next_state_batch)\n",
    "        loss = torch.mean(loss_vec)\n",
    "        return loss\n",
    "    \n",
    "    def normalise_reward(self, rewards_batch):\n",
    "        if rewards_batch.shape[0] == 1:\n",
    "            return rewards_batch\n",
    "        \n",
    "        mean_rewards = torch.abs(torch.mean(rewards_batch).view(rewards_batch.shape[0], 1))\n",
    "        norm_rewards = (rewards_batch - torch.min([0, mean_rewards])) / torch.max([1, mean_rewards.squeeze()] )\n",
    "        return norm_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d970537f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "surp_all_agent_percentages = []\n",
    "surp_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    time_num_episodes = 5000        \n",
    "    test_reward_period = 100\n",
    "    states_size = np.prod(env.observation_space.shape)\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = True)\n",
    "    \n",
    "    surprisal =  SurprisalModule(input_size = np.prod(env.observation_space.shape),  \n",
    "                                 action_size = env.action_space.n, \n",
    "                                 linear_size = 32, \n",
    "                                 hidden_size = 16)\n",
    "        \n",
    "    _, surp_states, surp_test_rewards, surp_ints, surp_percentages = train_with_reward(env,\n",
    "                                                                        agent,\n",
    "                                                                        seed,\n",
    "                                                                        surprisal,\n",
    "                                                                        intrinsic_weight = 1,\n",
    "                                                                        n_episodes=time_num_episodes,\n",
    "                                                                        test_reward_period = test_reward_period,\n",
    "                                                                        update_reward_period = 100,\n",
    "                                                                        batch_size =  150,\n",
    "                                                                        n_iter =  50,\n",
    "                                                                        std_dev = 0.0,\n",
    "                                                                        annealing = True)\n",
    "    surp_all_agent_percentages.append(surp_percentages)\n",
    "    surp_all_states.append(surp_states)\n",
    "        \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}surp_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, surp_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}surp_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, surp_all_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a856bf",
   "metadata": {},
   "source": [
    "# LP Surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad05a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "class LPSurprisalModule(SurprisalModule):\n",
    "        def __init__(self, input_size, action_size, linear_size,hidden_size,learning_progress_n=1, *args,  **kwargs):\n",
    "            super().__init__(input_size, action_size, linear_size, hidden_size, *args, **kwargs)\n",
    "            self._old_forward_models = deque(maxlen = learning_progress_n)\n",
    "            self._old_embedders = deque(maxlen = learning_progress_n)\n",
    "\n",
    "        def get_loss_vec(self, state_batch, action_batch, next_state_batch, old=False):\n",
    "            # If there haaven't been enough old models return 0\n",
    "            if old:\n",
    "                embedder = self._old_embedders[0]\n",
    "                model = self._old_forward_models[0]\n",
    "            else:\n",
    "                embedder = self.embedder\n",
    "                model = self.forward_model\n",
    "            \n",
    "            phi_s_batch = embedder(state_batch)\n",
    "            with T.no_grad():\n",
    "                phi_next_s_batch = embedder(next_state_batch)\n",
    "\n",
    "            mu, log_var = model.forward(phi_s_batch, action_batch)\n",
    "            dist = torch.distributions.MultivariateNormal(mu, T.diag_embed(T.exp(log_var)))\n",
    "            loss_vec = -dist.log_prob(phi_next_s_batch)\n",
    "            \n",
    "            return loss_vec\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def update_old_models(self):\n",
    "            old_model = deepcopy(self.forward_model)\n",
    "            old_model.eval()\n",
    "\n",
    "            old_embedder = deepcopy(self.embedder)\n",
    "            old_embedder.eval()\n",
    "            self._old_embedders.append(old_embedder)\n",
    "            self._old_forward_models.append(old_model)\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def get_intrinsic_reward(self, state, action, next_state):\n",
    "            if len(self._old_forward_models) == 0:\n",
    "                intrinsic_reward = torch.zeros(state.shape[0],)\n",
    "                return intrinsic_reward.squeeze()\n",
    "            else:\n",
    "                old_loss = self.get_loss_vec(state, action, next_state, old=True)\n",
    "                new_loss = self.get_loss_vec(state, action, next_state, old=False)\n",
    "                intrinsic_reward = self.normalise_reward(new_loss - old_loss)\n",
    "                return intrinsic_reward.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a42df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "lp_surp_all_agent_percentages = []\n",
    "lp_surp_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    time_num_episodes = 5000        \n",
    "    test_reward_period = 100\n",
    "    states_size = np.prod(env.observation_space.shape)\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = True)\n",
    "\n",
    "    \n",
    "    \n",
    "    lp_surprisal =  LPSurprisalModule(input_size = np.prod(env.observation_space.shape),  \n",
    "                                   action_size = env.action_space.n, \n",
    "                                   linear_size = 32, \n",
    "                                   hidden_size = 16,\n",
    "                                   learning_progress_n=10)\n",
    "\n",
    "    _, lp_surp_states, lp_surp_test_rewards, lp_surp_ints, lp_surp_percentages = train_with_reward(env,\n",
    "                                                                        agent,\n",
    "                                                                        seed, \n",
    "                                                                        surprisal,\n",
    "                                                                        intrinsic_weight = 1,\n",
    "                                                                        n_episodes=time_num_episodes,\n",
    "                                                                        test_reward_period = test_reward_period,\n",
    "                                                                        update_reward_period = 100,\n",
    "                                                                        batch_size =  150,\n",
    "                                                                        n_iter =  50,\n",
    "                                                                        std_dev = 0.0,\n",
    "                                                                        annealing = True)\n",
    "    lp_surp_all_agent_percentages.append(lp_surp_percentages)\n",
    "    lp_surp_all_states.append(lp_surp_states)\n",
    "        \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}lp_surp_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, lp_surp_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}lp_surp_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, lp_surp_all_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3e0d73",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prior_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "agent = QLearningAgent(epsilon = 0.5, \n",
    "                       alpha   = 0.5, \n",
    "                       discount=1, \n",
    "                       get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                       item = False)\n",
    "\n",
    "num_episodes = 1\n",
    "\n",
    "def get_random_states(env, num_episodes):\n",
    "    eps_states = []\n",
    "    for ep in tqdm(range(num_episodes)):    \n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        states = [] \n",
    "\n",
    "        while not done:\n",
    "            states.append(s)\n",
    "            i_s = get_state_number(s)\n",
    "            a = np.random.choice((0,1))\n",
    "            s_next, r, terminated, truncated, _  = env.step(a)\n",
    "            done = terminated or truncated\n",
    "            i_s_next = get_state_number(s_next)\n",
    "            agent.update(i_s, a, r, i_s_next, terminated)\n",
    "            s = s_next\n",
    "\n",
    "        eps_states.append(states)\n",
    "    return eps_states\n",
    "\n",
    "random_states = get_random_states(env, 100)\n",
    "\n",
    "flat_random_states = []\n",
    "for sublist in random_states:\n",
    "    flat_random_states.extend(sublist)\n",
    "    \n",
    "random_states_tensor = torch.tensor(flat_random_states, dtype=torch.float32)[:, None]\n",
    "\n",
    "print('Total Random States for Training:', len(random_states))\n",
    "print('Total Random States Tensor Shape:',  random_states_tensor.shape)\n",
    "\n",
    "trainloader = DataLoader(TensorDataset(random_states_tensor), batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, image_channels, input_shape):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.ratio = 1.5\n",
    "        self.input_shape = input_shape\n",
    "    \n",
    "        encoding_dim =1152 #24\n",
    "#         encoding_dim =4608 #48\n",
    "\n",
    "        self.enc1 = nn.Conv2d(in_channels=image_channels, out_channels=16, kernel_size=4, stride = 2, padding = 1)\n",
    "        self.enc2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4,stride = 2, padding = 1)\n",
    "\n",
    "        self.fc_mu = nn.Linear(encoding_dim, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(encoding_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_shapes = []\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x_shapes.append(x.shape)\n",
    "        \n",
    "        x = x.view(-1, x_shapes[0][1]*x_shapes[0][2]*x_shapes[0][3])  \n",
    "\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_log_var(x)\n",
    "        \n",
    "        return mu, log_var, x_shapes\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, image_channels, input_shape):\n",
    "        super(Decoder, self).__init__()\n",
    " \n",
    "        self.ratio = 1.5\n",
    "        self.input_shape = input_shape\n",
    "        decoding_dim = 1152 #24\n",
    "#         decoding_dim =4608 #48\n",
    "        \n",
    "        self.fc2  = nn.Linear(latent_dim,decoding_dim)\n",
    "        self.dec1 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=4, stride=2, padding=1)\n",
    "        #24\n",
    "        self.dec2 = nn.ConvTranspose2d(in_channels=16, out_channels=image_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        \n",
    "    def forward(self, z, x_shapes):\n",
    "        \n",
    "        z = self.fc2(z)\n",
    "        x = z.view(x_shapes[0])\n",
    "        x = F.relu(self.dec1(x))\n",
    "        recon = torch.sigmoid(self.dec2(x))\n",
    "        return recon\n",
    "    \n",
    "\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, prior, latent_dim):\n",
    "        super(ConvVAE, self).__init__()\n",
    " \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.prior = prior\n",
    "        self.latent_dim = latent_dim\n",
    "        self.criterion = nn.BCELoss(reduction='none')\n",
    "        \n",
    "        self.learned_mu = nn.Parameter(torch.randn(1,2))\n",
    "        self.learned_log_var = nn.Parameter(torch.randn(1,2))\n",
    "        \n",
    "    def log_prob_encoder(self, x, z):\n",
    "        mu, log_var, _ = self.encoder(x)\n",
    "        return log_normal_diag(z, mu, log_var, None, 1)\n",
    " \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var) \n",
    "        eps = torch.randn_like(std) \n",
    "        sample = mu + (eps * std)\n",
    "        return sample\n",
    "    \n",
    "    def reparameterize_reward(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var) \n",
    "        eps = torch.zeros(latent_dim).expand(mu.shape[0],latent_dim)\n",
    "        sample = mu + (eps * std)    \n",
    "        return  sample\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var, x_shapes = self.encoder(x)\n",
    "        \n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        recon = self.decoder(z, x_shapes)\n",
    "        recon_loss  = self.criterion(recon, x).sum()  #new sum\n",
    "        log_probs_p = self.log_prob_encoder(x, z) \n",
    "        log_probs_q = self.prior.log_prob(z)        \n",
    "        kld_loss    = (log_probs_p.sum(axis = 0) - log_probs_q.sum(axis = 0)).sum(-1)  #original vae   \n",
    "        loss = (recon_loss + kld_loss).mean()\n",
    "        return loss\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         mu, log_var, x_shapes = self.encoder(x)\n",
    "#         z = self.reparameterize(mu, log_var)\n",
    "#         recon = self.decoder(z, x_shapes)\n",
    "#         recon_loss = self.criterion(recon, x).sum([1,2,3]) \n",
    "#         print('recon', recon_loss.shape)\n",
    "\n",
    "#         log_probs_p = self.log_prob_encoder(x, z) \n",
    "#         log_probs_q = self.prior.log_prob(z)\n",
    "#         kld_loss = (log_probs_p.sum(axis = 0) - log_probs_q.sum(axis = 0)).sum(-1)\n",
    "#         print('kld', kld_loss.shape)\n",
    "#         loss = (recon_loss + kld_loss).mean()\n",
    "#         return loss\n",
    "    \n",
    "def plot_loss(all_losses):\n",
    "    plt.plot(all_losses)\n",
    "    plt.title('Average Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b034040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivergenceReward(BaseIntrinsicRewardModule):\n",
    "    \n",
    "    def __init__(self, vae):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vae = vae\n",
    "        \n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        observation_shape = int(np.sqrt(next_state.shape[1]))\n",
    "\n",
    "        next_state = next_state.view(-1, 1, observation_shape, observation_shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu, log_var, _ = self.vae.encoder(next_state)\n",
    "            z = self.vae.reparameterize_reward(mu, log_var)\n",
    "        \n",
    "            log_probs_p = self.vae.log_prob_encoder(next_state, z) \n",
    "            log_probs_q = self.vae.prior.log_prob(z)\n",
    "\n",
    "            KL = (log_probs_p.sum(axis = 0) - log_probs_q.sum(axis = 0)).sum(-1)\n",
    "            \n",
    "        return KL\n",
    "    \n",
    "    def get_standard_prob(self,range_lim):\n",
    "        x = np.linspace(-range_lim, range_lim)\n",
    "        return torch.tensor((1 / np.sqrt(2 * np.pi * 1 * 2)) * np.exp((-(x ) * 2 )/ (2 * 1 ** 2))).view(int(50/2),2)\n",
    "\n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):\n",
    "        \n",
    "        self.vae.train()        \n",
    "        observation_shape = int(np.sqrt(next_state_batch.shape[1]))\n",
    "        next_state_batch  = next_state_batch.view(-1, 1, observation_shape, observation_shape)\n",
    "        \n",
    "        mu, log_var, x_shapes = self.vae.encoder(next_state_batch)\n",
    "\n",
    "        z = self.vae.reparameterize(mu, log_var)\n",
    "        \n",
    "        recon = self.vae.decoder(z, x_shapes)\n",
    "\n",
    "        recon_loss = self.vae.criterion(recon, next_state_batch).sum()\n",
    "        \n",
    "        log_probs_p = self.vae.log_prob_encoder(next_state_batch, z) \n",
    "        log_probs_q = self.vae.prior.log_prob(z)\n",
    "        \n",
    "        kld_loss = (log_probs_p.sum(axis = 0) - log_probs_q.sum(axis = 0)).sum(-1)\n",
    "        loss = (recon_loss + kld_loss).mean()\n",
    "        \n",
    "        self.vae.eval()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce364305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "rows = env.observation_space.shape[0]\n",
    "cols = env.observation_space.shape[1]\n",
    "\n",
    "lr = 1e-3\n",
    "M  = 256\n",
    "D  = (1,rows, cols) \n",
    "num_vals   = 1\n",
    "latent_dim = 2\n",
    "batch_size = 32\n",
    "image_channels = 1 \n",
    "input_shape  = (3,rows, cols) \n",
    "\n",
    "prior_name = 'vampprior'\n",
    "\n",
    "encoder = Encoder(image_channels, input_shape)\n",
    "decoder = Decoder(image_channels, input_shape)\n",
    "prior = choose_prior(prior_name, latent_dim, M, D, num_vals, encoder, image_channels, device)\n",
    "vae   = ConvVAE(encoder, decoder, prior, latent_dim).to(device)    \n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "def VAE_train(x, model):\n",
    "    x = x.to(device)\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss = model(x)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.data.item()\n",
    "\n",
    "epochs = 20\n",
    "vae.train()\n",
    "all_losses = []\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):           \n",
    "    VAE_losses = []\n",
    "    for batch_idx, (x) in enumerate(trainloader):\n",
    "        x = x[0]\n",
    "        VAE_losses.append(VAE_train(x, vae))\n",
    "    \n",
    "    scheduler.step()\n",
    "    print('[%d/%d]: loss_VAE: %.3f' % ((epoch), epochs, torch.mean(torch.FloatTensor(VAE_losses))))\n",
    "    all_losses.append(torch.mean(torch.FloatTensor(VAE_losses)))\n",
    "plot_loss(all_losses)\n",
    "\n",
    "torch.save(vae.state_dict(), f'deepsea_vae_models/pre_trained_vae_{prior_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415f0b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "prior_name = 'vampprior'\n",
    "\n",
    "globals()[f\"{prior_name}_all_agent_percentages\"] = []\n",
    "globals()[f\"{prior_name}_all_states\"] = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    time_num_episodes = 5000        \n",
    "    test_reward_period = 100\n",
    "    states_size = np.prod(env.observation_space.shape)\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = True)\n",
    "\n",
    "    encoder = Encoder(image_channels, input_shape)\n",
    "    decoder = Decoder(image_channels, input_shape)\n",
    "    prior   = choose_prior(prior_name, latent_dim, M, D, num_vals, encoder, image_channels, device)\n",
    "    vae     = ConvVAE(encoder, decoder, prior, latent_dim).to(device) \n",
    "    vae.load_state_dict(torch.load(f'deepsea_vae_models/pre_trained_vae_{prior_name}.pth'))\n",
    "\n",
    "    kl_divergence = KLDivergenceReward(vae)\n",
    "    _, states, _, _,  percentages  = train_with_reward(env,\n",
    "                                                        agent,\n",
    "                                                        seed,\n",
    "                                                        kl_divergence,\n",
    "                                                        intrinsic_weight = 1,\n",
    "                                                        n_episodes=time_num_episodes,\n",
    "                                                        test_reward_period = test_reward_period,\n",
    "                                                        update_reward_period = 100,\n",
    "                                                        batch_size =  150,\n",
    "                                                        n_iter =  50,\n",
    "                                                        std_dev = 0.0, \n",
    "                                                        annealing = True)\n",
    "    \n",
    "    globals()[f\"{prior_name}_all_agent_percentages\"].append(percentages)\n",
    "    globals()[f\"{prior_name}_all_states\"].append(states)\n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}{prior_name}_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path,  globals()[f\"{prior_name}_all_agent_percentages\"])\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}{prior_name}_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, globals()[f\"{prior_name}_all_states\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb7e6ab",
   "metadata": {},
   "source": [
    "# Coin Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.orthogonal_(layer.weight, np.sqrt(2))\n",
    "        nn.init.constant_(layer.bias, 0,)\n",
    "    elif isinstance(layer, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(layer.weight)\n",
    "        nn.init.constant_(layer.bias, 0)\n",
    "        \n",
    "class CoinFlipNetwork(nn.Module):\n",
    "    def __init__(self, input_size:int, fc1:int, fc2:int, d:int=20):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(input_size, fc1),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(fc1, fc2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(fc2, d))\n",
    "\n",
    "        self.net.apply(layer_init)\n",
    "        self.prior_net = nn.Sequential(nn.Linear(input_size, fc1),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(fc1, fc2),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(fc2, d))\n",
    "        \n",
    "        self.prior_net.apply(layer_init)\n",
    "        self.prior_net.eval()\n",
    "        self.d = d\n",
    "        self.prior_mean = torch.zeros((1, self.d)).float()\n",
    "        self.prior_var = torch.ones((1, self.d)).float()\n",
    "        self.prior_n = 0\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def update_prior_mean_var(self, prior):\n",
    "        batch_size = prior.shape[0]\n",
    "        new_data_mean = prior.mean(axis=0)\n",
    "        \n",
    "        diff = new_data_mean - self.prior_mean\n",
    "        total_count = (self.prior_n + batch_size)\n",
    "\n",
    "        self.prior_mean = self.prior_mean + diff * batch_size / total_count\n",
    "\n",
    "        new_var = (self.prior_var * self.prior_n) + torch.var(prior, axis=0) * batch_size\n",
    "        new_var += torch.pow(diff, 2) * self.prior_n * batch_size / total_count\n",
    "        new_var /= total_count\n",
    "\n",
    "        self.prior_n = total_count\n",
    "        self.prior_var = new_var\n",
    "\n",
    "    def forward(self, x, update_prior_stats=True):\n",
    "        f_x = self.net(x)\n",
    "        prior_x = self.prior_net(x)\n",
    "        \n",
    "        if self.prior_n > 0:\n",
    "            prior_x_scaled = (prior_x - self.prior_mean) / self.prior_var\n",
    "        else:\n",
    "            prior_x_scaled = prior_x\n",
    "        \n",
    "        if self.training and update_prior_stats:\n",
    "            self.update_prior_mean_var(prior_x)\n",
    "            \n",
    "        return f_x, prior_x_scaled\n",
    "\n",
    "    def eval(self):\n",
    "        self.net.eval()\n",
    "\n",
    "    def train(self):\n",
    "        self.net.train()\n",
    "        \n",
    "class CoinFlipBuffer:\n",
    "    def __init__(self, size:int, obs_shape, d:int=20, alpha:float = 0.5, prioritise:bool=True):\n",
    "        self.obs_shape = obs_shape\n",
    "        self.d = d\n",
    "        self.size=size\n",
    "        self.alpha=alpha\n",
    "        self._init_buffers()\n",
    "        self.prioritise=prioritise\n",
    "\n",
    "    def _init_buffers(self):\n",
    "        self.obs_buffer = np.zeros((self.size, *self.obs_shape), dtype=np.float32)\n",
    "        self.coin_vec_buffer = np.zeros((self.size, self.d), dtype=np.float32)\n",
    "        self.n_updates = np.zeros(self.size, dtype=np.int32)\n",
    "        self.priorities = np.zeros(self.size, dtype=np.float32)\n",
    "        self._next_idx = -1\n",
    "        self.is_full=False\n",
    "\n",
    "    def add(self, obs_t, coin_vector, *args, **kwargs):\n",
    "        self._next_idx += 1\n",
    "        if self._next_idx == self.size-1:\n",
    "            self.is_full = True\n",
    "        elif self._next_idx == self.size:\n",
    "            self._next_idx = 0\n",
    "\n",
    "        obs = np.atleast_2d(np.array(obs_t, copy=False))\n",
    "        c = np.atleast_2d(np.array(coin_vector, dtype=np.int8))\n",
    "        self.obs_buffer[self._next_idx] = obs\n",
    "        self.coin_vec_buffer[self._next_idx] = c\n",
    "        self.n_updates[self._next_idx] = 1\n",
    "        self.priorities[self._next_idx] = 1.\n",
    "        \n",
    "    def sample(self):\n",
    "        batch_size=128\n",
    "        p = self.priorities / np.sum(self.priorities)\n",
    "        # assert p.sum()==1, (p.sum(), p[p>0])\n",
    "        if self.prioritise:\n",
    "            batch_idcs = np.random.choice(range(self.size), size=batch_size, p=p)\n",
    "        else:\n",
    "            batch_idcs = np.random.choice(range(self.size if self.is_full else self._next_idx), size=batch_size)\n",
    "        \n",
    "        self._last_batch_idcs = batch_idcs\n",
    "        \n",
    "        batch_obs = self.obs_buffer[batch_idcs]\n",
    "        batch_c = self.coin_vec_buffer[batch_idcs]\n",
    "\n",
    "        return batch_obs, batch_c\n",
    "\n",
    "    def update_priorities(self, f_batch):\n",
    "        assert f_batch.shape[1] == self.d\n",
    "        f_sq = np.array(f_batch.squeeze().detach().cpu())**2\n",
    "        inverse_counts = f_sq.sum(axis=1) / self.d\n",
    "        self.n_updates[self._last_batch_idcs] = self.n_updates[self._last_batch_idcs] + 1\n",
    "\n",
    "        self.priorities[self._last_batch_idcs] = self.alpha/(1+self.n_updates[self._last_batch_idcs]) + (1-self.alpha)*inverse_counts\n",
    "        \n",
    "        \n",
    "class CoinFlipReward(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, cfn: CoinFlipNetwork, buffer:CoinFlipBuffer, batch_size:int, d:int=20, bonus_exponent:float=0.5):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.bonus_exponent = bonus_exponent\n",
    "        self.cfn = cfn\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def flip_coins(self):\n",
    "        return 2*np.random.binomial(1, 0.5, size=self.d) - 1\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        f_next_state, prior_next_state = self.cfn(next_state, False) # (batch_size, d)\n",
    "        \n",
    "        # f = f_next_state\n",
    "        f = f_next_state + prior_next_state\n",
    "        # print(f_next_state, prior_next_state, f)\n",
    "        f_sq = torch.pow(f, 2)\n",
    "        reward = torch.mean(f_sq, axis=-1)\n",
    "        \n",
    "        reward = reward ** self.bonus_exponent\n",
    "        return reward[0]\n",
    "\n",
    "    def store(self, state, *args, **kwargs):\n",
    "        c_vector = self.flip_coins()\n",
    "        self.buffer.add(state, c_vector)\n",
    "\n",
    "    def get_loss(self, *args, **kwargs):\n",
    "        \n",
    "        states, coin_vecs = self.buffer.sample(self.batch_size)\n",
    "        states = torch.flatten(torch.tensor(states).float(), 1)\n",
    "        f_x, _ = self.cfn(states, True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            coin_vecs = torch.tensor(coin_vecs).float()\n",
    "            self.buffer.update_priorities(f_x)\n",
    "        \n",
    "        assert f_x.shape == coin_vecs.shape\n",
    "        loss = torch.pow(f_x - coin_vecs, 2).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1af2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "coin_flip_all_agent_percentages = []\n",
    "coin_flip_all_states = []\n",
    "    \n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    time_num_episodes = 5000     \n",
    "    test_reward_period = 100\n",
    "    hash_functions = 32\n",
    "    beta = 1\n",
    "    states_size = np.prod(env.observation_space.shape)\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = True)\n",
    "    \n",
    "    cfn    = CoinFlipNetwork(np.prod(env.observation_space.shape), 288, 288, 20)\n",
    "    buffer = CoinFlipBuffer\n",
    "    batch_size = 128\n",
    "    d = 20\n",
    "    bonus_exponent = 0.5\n",
    "\n",
    "    coin_flip = CoinFlipReward(cfn, buffer, batch_size, d, bonus_exponent)\n",
    "    \n",
    "    _, coin_flip_percentages, coin_flip_test_rewards, coin_flip_ints, coin_flip_percentages, _ = train_with_reward(env,\n",
    "                                                                        agent,\n",
    "                                                                        coin_flip,\n",
    "                                                                        intrinsic_weight = 1,\n",
    "                                                                        n_episodes=time_num_episodes,\n",
    "                                                                        test_reward_period = test_reward_period,\n",
    "                                                                        update_reward_period = 100,\n",
    "                                                                        batch_size =  150,\n",
    "                                                                        n_iter =  50,\n",
    "                                                                        std_dev = 0.0,\n",
    "                                                                        annealing = True)\n",
    "\n",
    "    coin_flip_all_agent_percentages.append(coin_flip_percentages)\n",
    "    coin_flip_all_states.append(coin_flip_percentages)\n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}coin_flip_count_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, coin_flip_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}coin_flip_count_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, coin_flip_all_states[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf74cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfn= CoinFlipNetwork(np.prod(env.observation_space.shape), 288, 288, 20)\n",
    "buffer=CoinFlipBuffer\n",
    "batch_size = 128\n",
    "d = 20\n",
    "bonus_exponent = 0.5\n",
    "\n",
    "coin_flip = CoinFlipReward(cfn, buffer, batch_size, d, bonus_exponent)\n",
    "\n",
    "state,_ = env.reset()\n",
    "action = env.action_space.sample()\n",
    "next_state, reward, done, info, _ = env.step(action)\n",
    "\n",
    "state_t  = torch.tensor(state).float().view(1, -1)\n",
    "action_t = torch.tensor(action).float().view(1, -1)\n",
    "next_state_t = torch.tensor(next_state).float().view(1, -1)\n",
    "\n",
    "int_rew = coin_flip.get_intrinsic_reward( state_t, action_t, next_state_t)\n",
    "int_loss = coin_flip.get_loss(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556b4f4",
   "metadata": {},
   "source": [
    "# BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d688ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA():\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "def update_moving_average(ema_updater, ma_model, current_model):\n",
    "    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "        old_weight, up_weight = ma_params.data, current_params.data\n",
    "        ma_params.data = ema_updater.update_average(old_weight, up_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf3fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CloseLoopRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CloseLoopRNNCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = nn.RNN(input_size, hidden_size, batch_first= True)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        new_hidden = self.rnn_cell(x, prev_hidden)\n",
    "        return new_hidden\n",
    "\n",
    "class OpenLoopRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(OpenLoopRNNCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = nn.RNN(input_size, hidden_size, batch_first= True)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        new_hidden = self.rnn_cell(x, prev_hidden)\n",
    "        return new_hidden\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class OnlineNetwork(nn.Module):\n",
    "    def __init__(self, state_size, embedding_size, history_size, open_loop_horizon, action_size):\n",
    "        super(OnlineNetwork, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.history_size = history_size\n",
    "        self.open_loop_horizon = open_loop_horizon\n",
    "\n",
    "        # Encoder f_theta\n",
    "        self.encoder = nn.Linear(state_size, embedding_size)\n",
    "\n",
    "        # Close-loop RNN cell h(c)_theta\n",
    "        self.close_loop_rnn_cell = CloseLoopRNNCell(embedding_size + action_size, history_size)\n",
    "\n",
    "        # Open-loop RNN cell h(o)_theta\n",
    "        self.open_loop_rnn_cell = OpenLoopRNNCell(embedding_size + action_size, history_size)\n",
    "\n",
    "        # Predictor g_theta\n",
    "        self.predictor = Predictor(history_size, embedding_size)\n",
    "    \n",
    "    def forward(self, observation, prev_action):\n",
    "        \n",
    "        # Encoder f_theta\n",
    "        observation_representation = self.encoder(observation)\n",
    "\n",
    "        # Close-loop RNN cell\n",
    "        close_loop_input  = torch.cat((observation_representation, prev_action), dim=1)\n",
    "        beta = torch.zeros(1, self.history_size)  # Initialize closed beta size 256\n",
    "        close_loop_hidden = self.close_loop_rnn_cell(close_loop_input, beta)\n",
    "\n",
    "        # Open-loop RNN celltr\n",
    "        open_loop_hidden = torch.zeros(1, self.history_size)  # Initialize open hidden state\n",
    "        open_loop_predictions = []\n",
    "        \n",
    "        for _ in range(self.open_loop_horizon):\n",
    "            open_loop_hidden     = self.open_loop_rnn_cell(close_loop_hidden, open_loop_hidden)\n",
    "            open_loop_prediction = self.predictor(open_loop_hidden)\n",
    "            open_loop_predictions.append(open_loop_prediction)\n",
    "        return open_loop_predictions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f44c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5 #temporal_window\n",
    "\n",
    "class F(): #encoder\n",
    "    ...\n",
    "    \n",
    "class G(): \n",
    "    ...\n",
    "    \n",
    "states = [state_1, state_2, ..., state_T]\n",
    "actions = [a_1,..., a_T]\n",
    "f = F(states)\n",
    "\n",
    "b = G(f[0], actions) # approximation of [F(state_2, state_3,...,state_T)]\n",
    "\n",
    "L = mse(b, f[1:]) #intrinsic reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9137472",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"bsuite/deep_sea-v0\", size=12, seed=42)\n",
    "    \n",
    "# hyperparameters\n",
    "action_size = 1\n",
    "state_size  = np.prod(env.observation_space.shape)\n",
    "hidden_size = 16\n",
    "history_size = 33\n",
    "embedding_size = 32\n",
    "open_loop_horizon = 5\n",
    "\n",
    "# agent steps\n",
    "state,_ = env.reset()\n",
    "action  = env.action_space.sample()\n",
    "s_next, r, terminated, truncated, _  = env.step(action)\n",
    "state_t  = torch.tensor(state).float().view(1, -1)\n",
    "action_t = torch.tensor(action).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "online_network = OnlineNetwork(state_size, embedding_size, history_size, open_loop_horizon, action_size)\n",
    "target_network = TargetNetwork(online_network, alpha=0.01)\n",
    "\n",
    "# Update target network parameters\n",
    "target_network.update_target_network()\n",
    "target_predictions = target_network.forward(observation, prev_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "env = gym.make(\"bsuite/deep_sea-v0\", size=12, seed=42)\n",
    "    \n",
    "# hyperparameters\n",
    "action_size = 1\n",
    "state_size  = np.prod(env.observation_space.shape)\n",
    "hidden_size = 16\n",
    "history_size = 33\n",
    "embedding_size = 32\n",
    "open_loop_horizon = 3\n",
    "\n",
    "# agent steps\n",
    "state,_ = env.reset()\n",
    "action  = env.action_space.sample()\n",
    "s_next, r, terminated, truncated, _  = env.step(action)\n",
    "state_t  = torch.tensor(state).float().view(1, -1)\n",
    "action_t = torch.tensor(action).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "online_network = OnlineNetwork(state_size, embedding_size, history_size, open_loop_horizon, action_size)\n",
    "online_predictions = online_network(state_t, action_t)\n",
    "online_predictions = torch.cat(online_predictions)\n",
    "\n",
    "norm_online_predictions = online_predictions / ((online_predictions**2)*0.5)\n",
    "#print(norm_online_predictions)\n",
    "#norm_online_predictions = online_predictions / ((online_predictions**2)*0.5)\n",
    "\n",
    "\n",
    "target_network = copy.deepcopy(online_network)\n",
    "target_projections = target_network.encoder(state_t)\n",
    "norm_target_projections = target_projections / ((target_projections**2)*0.5)\n",
    "\n",
    "loss = (norm_online_predictions - norm_target_projections.detach())**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8267cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "online_network = OnlineNetwork(embedding_size, history_size, open_loop_horizon, action_size)\n",
    "target_network = TargetNetwork(online_network, alpha=0.01)\n",
    "\n",
    "# Update target network parameters\n",
    "target_network.update_target_network()\n",
    "\n",
    "# Use target network for inference\n",
    "observation = torch.randn(1, embedding_size)\n",
    "target_predictions = target_network(observation)\n",
    "print(target_predictions)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d21887",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"bsuite/deep_sea-v0\", size=12, seed=42)\n",
    "\n",
    "time_num_episodes = 5000      \n",
    "test_reward_period = 100\n",
    "\n",
    "agent = QLearningAgent(alpha    = 0.5, \n",
    "                       epsilon  = 0.1,\n",
    "                       discount = 0.9,\n",
    "                       get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                       item = False)\n",
    "states_size = np.prod(env.observation_space.shape)\n",
    "action_size =  env.action_space.n\n",
    "hidden_size = 16\n",
    "embedding_size = 32\n",
    "\n",
    "byol = ObeservationEncoder(states_size, \n",
    "                           action_size,\n",
    "                           embedding_size, \n",
    "                           hidden_size)\n",
    "state,_ = env.reset()\n",
    "action = env.action_space.sample()\n",
    "s_next, r, terminated, truncated, _  = env.step(action)\n",
    "\n",
    "state_t  = torch.tensor(state).float().view(1, -1)\n",
    "\n",
    "latent_state = byol.forward(state_t)\n",
    "print(latent_state.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
