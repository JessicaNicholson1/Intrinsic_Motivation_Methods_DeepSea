{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e61dbcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessicanicholson/jupy/jup_notebook/lib/python3.10/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import bsuite\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from scipy import stats\n",
    "from scipy.stats import norm \n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b45864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions, item = True):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent\n",
    "        based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state,action)\n",
    "            which returns Q(state,action)\n",
    "          - self.set_qvalue(state,action,value)\n",
    "            which sets Q(state,action) := value\n",
    "        !!!Important!!!\n",
    "        Note: please avoid using self._qValues directly. \n",
    "            There's a special self.get_qvalue/set_qvalue for that.\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.item = item\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        Compute your agent's estimate of V(s) using current q-values\n",
    "        V(s) = max_over_action Q(state,action) over possible actions.\n",
    "        Note: please take into account that q-values can be negative.\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        value = max([self.get_qvalue(state, a) for a in possible_actions])\n",
    "        return value\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here:\n",
    "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "        \n",
    "        q = reward + gamma * (1 - done) * self.get_value(next_state)\n",
    "        q = (1 - learning_rate) * self.get_qvalue(state, action) + learning_rate * q\n",
    "        \n",
    "        if self.item:\n",
    "            self.set_qvalue(state, action, q.item())\n",
    "        else:\n",
    "            self.set_qvalue(state, action, q)\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values). \n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "                \n",
    "        idx = np.argmax([self.get_qvalue(state, a) for a in possible_actions])\n",
    "\n",
    "        return possible_actions[idx]\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.  \n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "\n",
    "        Note: To pick randomly from a list, use random.choice(list). \n",
    "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
    "              and compare it with your probability\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        action = None\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters:\n",
    "        epsilon = self.epsilon\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(possible_actions)\n",
    "        \n",
    "        return self.get_best_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a374bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        \n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        \n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward.item)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)            \n",
    "        return (\n",
    "            np.array(obses_t),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(obses_tp1),\n",
    "            np.array(dones)\n",
    "        )\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [ random.randint(0, len(self._storage) - 1)\n",
    "            for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "919e3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_number(s):\n",
    "    return np.argmax(s.flatten())\n",
    "\n",
    "def test_agent(agent, greedy=True, delay=.5):\n",
    "    v = get_all_states_value(agent)\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        fig, ax = plt.subplots(ncols=2)\n",
    "        ax[0].imshow(s)\n",
    "        ax[0].set_title('State')\n",
    "        im = ax[1].imshow(v)\n",
    "        plt.colorbar(im)\n",
    "        ax[1].set_title('Value function')\n",
    "        clear_output(True)\n",
    "        plt.show()\n",
    "        s = get_state_number(s)\n",
    "        \n",
    "        if greedy:\n",
    "            a = agent.get_best_action(s)\n",
    "        else:\n",
    "            a = agent.get_action(s)\n",
    "\n",
    "        s, r, terminated, truncated, _  = env.step(a)\n",
    "        done = terminated or truncated\n",
    "        sleep(delay)\n",
    "\n",
    "def get_all_states_value(agent):\n",
    "    s_shape = env.observation_space.shape\n",
    "    s_shape_flatten = np.prod(s_shape)\n",
    "    v = np.zeros(s_shape_flatten)\n",
    "    \n",
    "    for i in range(s_shape_flatten):\n",
    "        v[i] = agent.get_value(i)\n",
    "    v = v.reshape(s_shape)\n",
    "    return v\n",
    "\n",
    "def to_one_hot(x, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    x = x.long().view(-1, 1)\n",
    "    x = torch.zeros(x.size()[0], ndims).scatter_(1, x, 1)\n",
    "    return x\n",
    "\n",
    "def freeze_test_agent(agent, test_episodes):\n",
    "    \n",
    "    all_rewards = []\n",
    "    for i in range(test_episodes):\n",
    "        eps_rewards = 0\n",
    "        v    = get_all_states_value(agent)\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            s = get_state_number(s)\n",
    "            a = agent.get_best_action(s)\n",
    "            n_s, r, terminated, truncated, _  = env.step(a)\n",
    "            eps_rewards+=r\n",
    "            done = terminated or truncated\n",
    "            s = n_s\n",
    "        all_rewards.append(eps_rewards)\n",
    "    \n",
    "    return all_rewards\n",
    "\n",
    "def average_first_visits(episode):\n",
    "    \n",
    "    s_shape = env.observation_space.shape\n",
    "    s_shape_flatten = np.prod(s_shape)\n",
    "    available_states = {i: None for i in range(s_shape_flatten)}\n",
    "\n",
    "    states = [np.argmax(state) for state in episode]\n",
    "    states\n",
    "    first_visits = {}\n",
    "    for timestep, state in enumerate(states):\n",
    "        if state not in first_visits:\n",
    "            first_visits[state] = timestep\n",
    "\n",
    "    not_visited_states = [state for state in available_states if state not in first_visits]\n",
    "\n",
    "    num_steps = env.observation_space.shape[0]\n",
    "    for state in not_visited_states:\n",
    "            first_visits[state] = 1000  \n",
    "\n",
    "    new_sea = np.zeros(s_shape_flatten)\n",
    "\n",
    "    for state, first_visit in first_visits.items():\n",
    "        new_sea[state] = first_visit\n",
    "    \n",
    "    return new_sea\n",
    "\n",
    "def visualize_single_run(episode, agent_states):\n",
    "    \n",
    "    s_shape = env.observation_space.shape\n",
    "\n",
    "    for states in range(episode):\n",
    "        new_sea = average_first_visits(agent_states[states])\n",
    "\n",
    "        if states == 0:\n",
    "            seas = new_sea\n",
    "        else:\n",
    "            seas = np.vstack([seas, new_sea])\n",
    "\n",
    "    average_visits = np.mean(seas, axis = 0)        \n",
    "    \n",
    "    return average_visits\n",
    "\n",
    "    plt.imshow(average_visits.reshape(s_shape), cmap='Spectral')\n",
    "    plt.title(f'Average First-visit visualizations\\n{episode} Episodes')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def visualize_multiple_runs(all_episodes, epsilon, agent_states, learning_agent, num_episodes):\n",
    "    s_shape = env.observation_space.shape\n",
    "    all_average_visits = []\n",
    "\n",
    "    for i, episode in enumerate(all_episodes):\n",
    "        seas = None \n",
    "        for states in range(episode):\n",
    "            new_sea = average_first_visits(agent_states[states])\n",
    "\n",
    "            if states == 0:\n",
    "                seas = new_sea\n",
    "            else:\n",
    "                seas = np.vstack([seas, new_sea])\n",
    "\n",
    "        average_visits = np.mean(seas, axis=0)\n",
    "        all_average_visits.append(average_visits)\n",
    "\n",
    "    print(f'average_first_visits {learning_agent} Agent for {num_episodes} Episodes')\n",
    "    print('-----------------------------------------')\n",
    "    \n",
    "    _, axarr1 = plt.subplots(1,len(all_episodes), figsize = (15,15))\n",
    "    axarr1[0].set_ylabel(f'Epsilon: {epsilon}')\n",
    "\n",
    "    for i in range(len(all_average_visits)):\n",
    "\n",
    "        axarr1[i].imshow(all_average_visits[i].reshape(s_shape), cmap='Spectral')\n",
    "\n",
    "        axarr1[i].set_xticks([])\n",
    "        axarr1[i].set_yticks([])\n",
    "        axarr1[i].set_title(f'{all_episodes[i]} Episodes')\n",
    "\n",
    "        if i > len(all_episodes)-2:\n",
    "            last_subplot = axarr1[-1]\n",
    "            cax = last_subplot.imshow(all_average_visits[i].reshape(s_shape), cmap='RdYlBu') \n",
    "            plt.colorbar(cax, ax=last_subplot,  fraction=0.05)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def average_test_rewards(rewards):\n",
    "    \n",
    "    all_avgs = []\n",
    "\n",
    "    for row in zip(rewards[0], rewards[1], rewards[2]):\n",
    "\n",
    "        new_rows = []\n",
    "        for i in zip(row[0], row[1], row[2]):\n",
    "            m = np.mean(i)\n",
    "            new_rows.append(m)\n",
    "        all_avgs.append(new_rows)\n",
    "    all_avgs\n",
    "    \n",
    "    return all_avgs\n",
    "\n",
    "def flatten_rewards(test_rewards):\n",
    "    flat_test_rewards = []\n",
    "    for sublist in test_rewards:\n",
    "        flat_test_rewards.extend(sublist)\n",
    "    return flat_test_rewards\n",
    "    \n",
    "    \n",
    "def state_visitation_percentage(env, episodes, states, learning_agent):\n",
    "    # Count all available states in env \n",
    "    all_states = np.prod(env.observation_space.shape)\n",
    "    diagonal = env.observation_space.shape[0]/2\n",
    "    available_states = all_states/2 + diagonal\n",
    "\n",
    "    # Get full list of visited states\n",
    "    flat_states = []\n",
    "    for sublist in states:\n",
    "        flat_states.extend(sublist)\n",
    "\n",
    "    # Get location of visited states    \n",
    "    state_loc = []\n",
    "    for i in flat_states:\n",
    "        state = np.argmax(i)\n",
    "        if state != 0:\n",
    "            state_loc.append(state)\n",
    "            \n",
    "    # Count the unique visited states\n",
    "    visited_states = set(state_loc)\n",
    "    num_visited_states = len(visited_states)\n",
    "    \n",
    "    # Calculate the percentage of visited states\n",
    "    percentage_visited = (num_visited_states / available_states) * 100\n",
    "\n",
    "    print(f\"% of total states visited/ {episodes} Eps: {percentage_visited:.2f}%  {learning_agent} \")\n",
    "    \n",
    "    return percentage_visited\n",
    "\n",
    "def get_state_visitation_percentage(env, states):\n",
    "    # Count all available states in env \n",
    "    all_states = np.prod(env.observation_space.shape)\n",
    "    diagonal = env.observation_space.shape[0]/2\n",
    "    available_states = all_states/2 + diagonal\n",
    "\n",
    "    # Get full list of visited states\n",
    "    flat_states = []\n",
    "    for sublist in states:\n",
    "        flat_states.extend(sublist)\n",
    "\n",
    "    # Get location of visited states    \n",
    "    state_loc = []\n",
    "    for i in flat_states:\n",
    "        state = np.argmax(i)\n",
    "        if state != 0:\n",
    "            state_loc.append(state)\n",
    "            \n",
    "    # Count the unique visited states\n",
    "    visited_states = set(state_loc)\n",
    "    num_visited_states = len(visited_states)\n",
    "    \n",
    "    # Calculate the percentage of visited states\n",
    "    percentage_visited = (num_visited_states / available_states) * 100\n",
    "    \n",
    "    return percentage_visited\n",
    "\n",
    "\n",
    "def plot_ftv_histograms(states):\n",
    "    s_shape = env.observation_space.shape\n",
    "    s_shape_flatten = np.prod(s_shape)\n",
    "    available_states = {i: 0 for i in range(s_shape_flatten)}\n",
    "\n",
    "    all_flat_states = []\n",
    "    for agent_states in states:\n",
    "        flat_states = []\n",
    "        for sublist in agent_states:\n",
    "            flat_states.extend(sublist)\n",
    "        all_flat_states.append(flat_states)\n",
    "\n",
    "    merged_dicts = []\n",
    "    for run in range(len(all_flat_states)):\n",
    "\n",
    "        first_visits = {}\n",
    "        for timestep, state in enumerate(all_flat_states[run]):\n",
    "            if state not in first_visits:\n",
    "                first_visits[state] = timestep\n",
    "\n",
    "        merged_dict = {key: first_visits[key] if key in first_visits else available_states[key] for key in available_states}\n",
    "        merged_dicts.append(merged_dict)\n",
    "\n",
    "    arr = np.array([list(d.values()) for d in merged_dicts])\n",
    "\n",
    "    mean_values = np.mean(arr, axis=0)\n",
    "\n",
    "    mean_dict = {key: int(mean_values[i]) for i, key in enumerate(merged_dicts[0].keys())}\n",
    "\n",
    "    values = list(mean_dict.values())\n",
    "\n",
    "    grid_size = env.observation_space.shape[0]\n",
    "    state_values = [[i + j*grid_size + 1 for i in range(grid_size)] for j in range(grid_size)]\n",
    "    lower_diagonal_values = []\n",
    "\n",
    "    for row in range(grid_size):\n",
    "        for col in range(grid_size):\n",
    "            if row >= col:\n",
    "                lower_diagonal_values.append(state_values[row][col] - 1)  # Subtract 1 here\n",
    "\n",
    "    updated_dict = {state: mean_dict[state] for state in lower_diagonal_values}\n",
    "\n",
    "    for key, val in updated_dict.items():\n",
    "        if val == 0:\n",
    "            updated_dict[key] = 50000\n",
    "\n",
    "    return updated_dict.values()\n",
    "\n",
    "\n",
    "def plot_histograms(all_is, label, color):\n",
    "    data = list(plot_ftv_histograms(all_is))\n",
    "    mu, std = norm.fit(data)\n",
    "    sns.histplot(data, bins=20, kde=True, color = color)  \n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    plt.plot(x, p, 'k', linewidth=2, label = label, color=color)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee11ee0",
   "metadata": {},
   "source": [
    "# Epsilon-Annealing Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_annealing(env, agent, seed, num_episodes, test_reward_period, initial_epsilon, final_epsilon):\n",
    "    all_states  = []\n",
    "    all_test_rewards = []\n",
    "    all_percentages = []\n",
    "    \n",
    "    test_n = 0\n",
    "    target_value = 0.99\n",
    "\n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        s, _ = env.reset(seed=seed)\n",
    "        done = False\n",
    "        episode_rewards = 0\n",
    "        episode_states  = []\n",
    "        episode_is = []\n",
    "        epsilon = max(final_epsilon, initial_epsilon * (1 - i / num_episodes))\n",
    "\n",
    "        while not done:\n",
    "            episode_states.append(s)\n",
    "            i_s = get_state_number(s)\n",
    "            episode_is.append(i_s)\n",
    "            \n",
    "            if np.random.rand() < epsilon:\n",
    "                a = env.action_space.sample()  # Exploration\n",
    "            else:\n",
    "                i_s = get_state_number(s)\n",
    "                a = agent.get_action(i_s)  # Exploitation\n",
    "\n",
    "            s_next, r, terminated, truncated, _ = env.step(a)\n",
    "            episode_rewards += r\n",
    "            done = terminated or truncated\n",
    "            i_s_next = get_state_number(s_next)\n",
    "            agent.update(i_s, a, r, i_s_next, terminated)\n",
    "            s = s_next\n",
    "\n",
    "        all_states.append(episode_states)\n",
    "        \n",
    "        if i % test_reward_period == 0:\n",
    "            test_n += 1\n",
    "            tests = freeze_test_agent(agent, 10)\n",
    "            all_test_rewards.append(np.mean(tests))\n",
    "            \n",
    "            percentages = get_state_visitation_percentage(env, all_states)\n",
    "            all_percentages.append(percentages)\n",
    "\n",
    "    return all_states, all_test_rewards, all_percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "learning_agent = 'qlearning'\n",
    "\n",
    "anneal_all_agent_percentages = []\n",
    "anneal_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    initial_epsilon = 1.0\n",
    "    final_epsilon = 0.1\n",
    "    epsilon = 0.1\n",
    "    num_episodes = 5000       \n",
    "    test_reward_period = 100\n",
    "\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = False)\n",
    "\n",
    "    anneal_states, anneal_test_rew, anneal_percentages, = epsilon_annealing(env,\n",
    "                                                                             agent,\n",
    "                                                                             seed, \n",
    "                                                                             num_episodes,\n",
    "                                                                             test_reward_period,\n",
    "                                                                             initial_epsilon,\n",
    "                                                                             final_epsilon)\n",
    "    \n",
    "    anneal_all_agent_percentages.append(anneal_percentages)\n",
    "    anneal_all_states.append(anneal_states)\n",
    "    \n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}q_learning_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, anneal_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}q_learning_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, anneal_all_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72d8e6",
   "metadata": {},
   "source": [
    "# Intrinsic Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f29883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseIntrinsicRewardModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        return 0.0\n",
    "\n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c335fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        def init_weights(tensor):\n",
    "            if isinstance(tensor, nn.Linear):\n",
    "                nn.init.xavier_uniform_(tensor.weight)\n",
    "        self.layers.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_reward(env, agent, seed, reward_module, intrinsic_weight, n_episodes, test_reward_period,\n",
    "                      update_reward_period, batch_size, n_iter, std_dev, annealing = True):\n",
    "    \n",
    "    buffer = ReplayBuffer(size=int(1e6))\n",
    "    \n",
    "    if list(reward_module.parameters()):\n",
    "        optimizer = torch.optim.Adam(reward_module.parameters())\n",
    "    else:\n",
    "        optimizer = None\n",
    "                \n",
    "    losses = []\n",
    "    \n",
    "    previous_mean = 0\n",
    "    rolling_mean_window = 50\n",
    "    rolling_means = [] \n",
    "    test_n = 0\n",
    "    target_value = 0.99\n",
    "    \n",
    "    all_test_rewards = []    \n",
    "    all_intrinsic_rewards = []\n",
    "    all_rewards  = []\n",
    "    all_states = []\n",
    "    all_percentages = []\n",
    "    \n",
    "    for i in tqdm(range(n_episodes)):\n",
    "        s, _   = env.reset(seed=seed)\n",
    "\n",
    "        done = False\n",
    "        episode_rewards = 0\n",
    "        episode_states  = []\n",
    "        episodes_int_rews = []\n",
    "        episode_is = []\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            steps+=1\n",
    "            \n",
    "            episode_states.append(s)\n",
    "            i_s = get_state_number(s)\n",
    "            episode_is.append(i_s)\n",
    "            \n",
    "            a = agent.get_action(i_s)\n",
    "            s_next, r, terminated, truncated, _  = env.step(a)\n",
    "            episode_rewards += r\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            i_s_next = get_state_number(s_next)\n",
    "            state_t  = torch.tensor(s).float().view(1, -1)        \n",
    "            action_t = torch.tensor(a).float().view(1, -1)\n",
    "            next_state_t = torch.tensor(s_next).float().view(1, -1)\n",
    "            \n",
    "            r_intr = intrinsic_weight * reward_module.get_intrinsic_reward(state_t, action_t, next_state_t)\n",
    "            episodes_int_rews.append(r_intr)\n",
    "            r += r_intr\n",
    "\n",
    "            agent.update(i_s, a, r, i_s_next, terminated)\n",
    "            buffer.add(state_t, a, r, next_state_t, terminated)\n",
    "            \n",
    "            s = s_next\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "                \n",
    "        all_rewards.append(episode_rewards)\n",
    "        all_states.append(episode_states)\n",
    "        all_intrinsic_rewards.append(np.mean(episodes_int_rews))\n",
    "        \n",
    "        \"\"\"Anneal Intrinsic Reward\"\"\"\n",
    "        if annealing:\n",
    "            if (i +1) % 50 == 0:\n",
    "                current_mean  = np.mean(all_rewards[-rolling_mean_window:])\n",
    "                if current_mean > previous_mean:\n",
    "                    intrinsic_weight *= 0.9\n",
    "                previous_mean = current_mean\n",
    "        \n",
    "        \"\"\"Freeze & Test Policy\"\"\"\n",
    "        if (i+1) % test_reward_period == 0:\n",
    "            test_n+=1\n",
    "            test_rewards = freeze_test_agent(agent,10)\n",
    "            all_test_rewards.append(np.mean(test_rewards))\n",
    "            \n",
    "            percentages = get_state_visitation_percentage(env, all_states)\n",
    "            all_percentages.append(percentages)\n",
    "\n",
    "\n",
    "        \"\"\"Update Intrinsic Module\"\"\"\n",
    "        if (i + 1) % update_reward_period == 0 and optimizer is not None:\n",
    "        \n",
    "            for _ in range(n_iter):\n",
    "                optimizer.zero_grad()\n",
    "                state_batch, action_batch, _, next_state_batch, _ = buffer.sample(batch_size)\n",
    "                state_tensor  = torch.tensor(state_batch).float().flatten(1, 2)\n",
    "\n",
    "                action_tensor = torch.tensor(action_batch).float().view(-1, 1)\n",
    "                next_state_tensor = torch.tensor(next_state_batch).float().flatten(1, 2)\n",
    "                                \n",
    "                loss = reward_module.get_loss(state_tensor, action_tensor, next_state_tensor)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "                    \n",
    "    return all_rewards, all_states, all_test_rewards, all_intrinsic_rewards, all_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a167001",
   "metadata": {},
   "source": [
    "# RND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90dfd1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class RandomNetworkDistilationModule(BaseIntrinsicRewardModule):\n",
    "    \n",
    "    def __init__(self, states_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.target_network = MLP(states_size,\n",
    "                                  hidden_size,\n",
    "                                  embedding_size)\n",
    "        \n",
    "        self.predictor_network = MLP(states_size,\n",
    "                                     hidden_size,\n",
    "                                     embedding_size)\n",
    "        self.target_network.eval()\n",
    "        \n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        with torch.no_grad():\n",
    "            target_embedding = self.target_network(next_state)\n",
    "            predictor_embedding = self.predictor_network(next_state)\n",
    "            intrinsic_reward = ((target_embedding - predictor_embedding) ** 2).sum()\n",
    "        return intrinsic_reward\n",
    "\n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_embedding  = self.target_network(next_state_batch)\n",
    "            \n",
    "        predictor_embedding = self.predictor_network(next_state_batch)\n",
    "        loss    = 0.5*((target_embedding - predictor_embedding) ** 2).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34ef04",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "rnd_all_agent_percentages = []\n",
    "rnd_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    time_num_episodes = 5000      \n",
    "    test_reward_period = 100\n",
    "\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = 0.1,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = False)\n",
    "\n",
    "    rnd = RandomNetworkDistilationModule(np.prod(env.observation_space.shape), \n",
    "                                         np.prod(env.observation_space.shape), \n",
    "                                         16)\n",
    "\n",
    "    rnd_rewards, rnd_states, rnd_test_rewards, rnd_ints,  rnd_percentages = train_with_reward(env,\n",
    "                                                                                agent,\n",
    "                                                                                seed,\n",
    "                                                                                rnd,\n",
    "                                                                                intrinsic_weight = 1,\n",
    "                                                                                n_episodes=time_num_episodes,\n",
    "                                                                                test_reward_period = test_reward_period,\n",
    "                                                                                update_reward_period = 100,\n",
    "                                                                                batch_size =  150,\n",
    "                                                                                n_iter =  50,\n",
    "                                                                                std_dev = 0.0, \n",
    "                                                                                annealing = True)\n",
    "                    \n",
    "    rnd_all_agent_percentages.append(rnd_percentages)\n",
    "    rnd_all_states.append(rnd_states)\n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}rnd_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, rnd_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}rnd_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, rnd_all_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79b9158",
   "metadata": {},
   "source": [
    "# ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf7716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, states_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.module = MLP(states_size,\n",
    "                          hidden_size,\n",
    "                          embedding_size)\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.module(s)\n",
    "    \n",
    "class ForwardDynamics(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, states_size, actions_size, hidden_size, alpha=.1):\n",
    "        super().__init__()\n",
    "        self.module = MLP(actions_size + states_size,\n",
    "                          hidden_size,\n",
    "                          states_size)\n",
    "        self.alpha = alpha\n",
    "        self.mean_reward = 0\n",
    "    \n",
    "    def forward(self, s, a):\n",
    "        sa = torch.cat([s, a], dim=-1)\n",
    "        return s + self.module(sa)\n",
    "    \n",
    "    \n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):\n",
    "        predicted_next_states = self.forward(state_batch, action_batch)\n",
    "        loss = F.mse_loss(predicted_next_states, next_state_batch)\n",
    "        return loss\n",
    "    \n",
    "class InverseDynamics(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, states_size, actions_size, hidden_size, alpha=0.1):\n",
    "        super().__init__()\n",
    "        self.module = MLP(2 * states_size,\n",
    "                          hidden_size,\n",
    "                          actions_size)\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.mean_reward = 0\n",
    "        self.actions_size = actions_size\n",
    "    \n",
    "    def forward(self, s, s_next):\n",
    "        combined_state = torch.cat((s, s_next), dim=-1)\n",
    "        a_pred_proba = self.module(combined_state)\n",
    "        return torch.softmax(a_pred_proba, dim=-1)    \n",
    "\n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):        \n",
    "        a_pred_proba = self.forward(state_batch, next_state_batch)\n",
    "        a_one_hot    = to_one_hot(action_batch, self.actions_size)\n",
    "        return -(torch.log(a_pred_proba) * a_one_hot).sum(dim=-1).mean()\n",
    "\n",
    "    \n",
    "class ICMModule(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, states_size, actions_size, hidden_size, embedding_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.forward_model = ForwardDynamics(embedding_size,\n",
    "                                             actions_size = 1, \n",
    "                                             hidden_size = hidden_size)\n",
    "        self.inverse_model = InverseDynamics(embedding_size,\n",
    "                                             actions_size, \n",
    "                                             hidden_size, \n",
    "                                             alpha=0.1)\n",
    "        self.embedder = Embedder(states_size, \n",
    "                                 embedding_size, \n",
    "                                 hidden_size)\n",
    "        self.n = 1\n",
    "        self.mean_reward = 0\n",
    "        self.actions_size = actions_size\n",
    "        \n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        with torch.no_grad():       \n",
    "            phi_hat_s      = self.embedder.forward(state)\n",
    "            phi_hat_next_s = self.embedder.forward(next_state)\n",
    "            phi_pred_next_state = self.forward_model.forward(phi_hat_s, action)\n",
    "            intrinsic_reward = 0.3*(phi_pred_next_state - phi_hat_next_s).pow(2).mean()\n",
    "        return intrinsic_reward\n",
    "    \n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):\n",
    "        phi_s_batch      = self.embedder(state_batch)\n",
    "        phi_next_s_batch = self.embedder(next_state_batch)\n",
    "        forward_loss   = self.forward_model.get_loss(phi_s_batch.detach(), action_batch, phi_next_s_batch.detach())\n",
    "        inverse_loss   = self.inverse_model.get_loss(phi_s_batch, action_batch, phi_next_s_batch)\n",
    "        intrinsic_loss = forward_loss + inverse_loss\n",
    "        \n",
    "        return intrinsic_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee7c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "icm_all_agent_percentages = []\n",
    "icm_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    time_num_episodes = 5000       \n",
    "    test_reward_period = 100\n",
    "\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = 0.1,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = False)\n",
    "\n",
    "    icm = ICMModule(states_size = np.prod(env.observation_space.shape), \n",
    "                    actions_size   = env.action_space.n, \n",
    "                    hidden_size = 16,\n",
    "                    embedding_size = 32)\n",
    "\n",
    "    _, icm_states, icm_test_rewards, icm_ints, icm_percentages = train_with_reward(env,\n",
    "                                                                        agent,\n",
    "                                                                        seed, \n",
    "                                                                        icm,\n",
    "                                                                        intrinsic_weight = 1,\n",
    "                                                                        n_episodes=time_num_episodes,\n",
    "                                                                        test_reward_period = test_reward_period,\n",
    "                                                                        update_reward_period = 100,\n",
    "                                                                        batch_size =  150,\n",
    "                                                                        n_iter =  50,\n",
    "                                                                        std_dev = 0.0, \n",
    "                                                                        annealing = True)\n",
    "    icm_all_agent_percentages.append(icm_percentages)\n",
    "    icm_all_states.append(icm_states)\n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}icm_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, icm_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}icm_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, icm_all_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b375ff",
   "metadata": {},
   "source": [
    "# Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef84b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountBasedModule(BaseIntrinsicRewardModule):\n",
    "    \n",
    "    def __init__(self, hash_functions, states_size, beta):\n",
    "        super().__init__()\n",
    "        self.hash = {}\n",
    "        self.A = np.random.normal(0, 1, (hash_functions, states_size))\n",
    "        self.beta = beta\n",
    "        \n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        counts = []\n",
    "        state = state.cpu().numpy()[0]\n",
    "        hash_values = np.dot(self.A, state)\n",
    "        key  = str(np.sign(hash_values))\n",
    "        \n",
    "        if key in self.hash:\n",
    "                self.hash[key] += 1\n",
    "        else:\n",
    "            self.hash[key] = 1\n",
    "\n",
    "        counts.append(self.hash[key])    \n",
    "        \n",
    "        intrinsic_reward = beta / np.sqrt(counts[0])\n",
    "        \n",
    "        return torch.tensor(intrinsic_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee6a4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "count_all_agent_percentages = []\n",
    "count_all_states = []\n",
    "    \n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    time_num_episodes = 5000     \n",
    "    test_reward_period = 100\n",
    "    hash_functions = 32\n",
    "    beta = 1\n",
    "    states_size = np.prod(env.observation_space.shape)\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = True)\n",
    "\n",
    "    count = CountBasedModule(hash_functions, states_size, beta)\n",
    "    \n",
    "    count_rewards, count_states, count_test_rewards, count_ints, count_percentages = train_with_reward(env,\n",
    "                                                                        agent,\n",
    "                                                                        seed,\n",
    "                                                                        count,\n",
    "                                                                        intrinsic_weight = 1,\n",
    "                                                                        n_episodes=time_num_episodes,\n",
    "                                                                        test_reward_period = test_reward_period,\n",
    "                                                                        update_reward_period = 100,\n",
    "                                                                        batch_size =  150,\n",
    "                                                                        n_iter =  50,\n",
    "                                                                        std_dev = 0.0,\n",
    "                                                                        annealing = True)\n",
    "    count_all_agent_percentages.append(count_percentages)\n",
    "    count_all_states.append(count_states)\n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}count_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, count_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}count_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, count_all_states[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326448f3",
   "metadata": {},
   "source": [
    "# RIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d46a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, states_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.module = MLP(states_size,\n",
    "                          hidden_size,\n",
    "                          embedding_size)\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.module(s)\n",
    "    \n",
    "class ForwardDynamics(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, states_size, actions_size, hidden_size, alpha=.1):\n",
    "        super().__init__()\n",
    "        self.module = MLP(actions_size + states_size,\n",
    "                          hidden_size,\n",
    "                          states_size)\n",
    "        self.alpha = alpha\n",
    "        self.mean_reward = 0\n",
    "    \n",
    "    def forward(self, s, a):\n",
    "        sa = torch.cat([s, a], dim=-1)\n",
    "        return s + self.module(sa)\n",
    "    \n",
    "    \n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):\n",
    "        predicted_next_states = self.forward(state_batch, action_batch)\n",
    "        loss = F.mse_loss(predicted_next_states, next_state_batch)\n",
    "        return loss\n",
    "    \n",
    "class InverseDynamics(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, states_size, actions_size, hidden_size, alpha=0.1):\n",
    "        super().__init__()\n",
    "        self.module = MLP(2 * states_size,\n",
    "                          hidden_size,\n",
    "                          actions_size)\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.mean_reward = 0\n",
    "        self.actions_size = actions_size\n",
    "    \n",
    "    def forward(self, s, s_next):\n",
    "        combined_state = torch.cat((s, s_next), dim=-1)\n",
    "        a_pred_proba = self.module(combined_state)\n",
    "        return torch.softmax(a_pred_proba, dim=-1)    \n",
    "\n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):        \n",
    "        a_pred_proba = self.forward(state_batch, next_state_batch)\n",
    "        a_one_hot    = to_one_hot(action_batch, self.actions_size)\n",
    "        return -(torch.log(a_pred_proba) * a_one_hot).sum(dim=-1).mean()\n",
    "\n",
    "    \n",
    "class RIDEModule(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, states_size, actions_size, hidden_size, embedding_size, hash_functions, beta):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.forward_model = ForwardDynamics(embedding_size,\n",
    "                                             actions_size = 1, \n",
    "                                             hidden_size = hidden_size)\n",
    "        self.inverse_model = InverseDynamics(embedding_size,\n",
    "                                             actions_size, \n",
    "                                             hidden_size, \n",
    "                                             alpha=0.1)\n",
    "        self.embedder = Embedder(states_size, \n",
    "                                 embedding_size, \n",
    "                                 hidden_size)\n",
    "        self.count_model =CountBasedModule(hash_functions, \n",
    "                                           states_size, \n",
    "                                           beta)\n",
    "\n",
    "        self.n = 1\n",
    "        self.mean_reward = 0\n",
    "        self.actions_size = actions_size\n",
    "        \n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        \n",
    "        count_rewards = self.count_model.get_intrinsic_reward(state, action, next_state)\n",
    "\n",
    "        with torch.no_grad():       \n",
    "            state_emb      = self.embedder.forward(state)\n",
    "            next_state_emb = self.embedder.forward(next_state)\n",
    "            \n",
    "            control_rewards  = (next_state_emb - state_emb).pow(2).mean()\n",
    "            intrinsic_reward = count_rewards*control_rewards\n",
    "\n",
    "        return intrinsic_reward\n",
    "    \n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):\n",
    "        phi_s_batch      = self.embedder(state_batch)\n",
    "        phi_next_s_batch = self.embedder(next_state_batch)\n",
    "        forward_loss   = self.forward_model.get_loss(phi_s_batch.detach(), action_batch, phi_next_s_batch.detach())\n",
    "        inverse_loss   = self.inverse_model.get_loss(phi_s_batch, action_batch, phi_next_s_batch)\n",
    "        intrinsic_loss = forward_loss + inverse_loss\n",
    "        \n",
    "        return intrinsic_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "ride_all_agent_percentages = []\n",
    "ride_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    time_num_episodes = 5000        \n",
    "    test_reward_period = 100\n",
    "    hash_functions = 32\n",
    "    beta = 1\n",
    "    states_size = np.prod(env.observation_space.shape)\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = True)\n",
    "\n",
    "    ride = RIDEModule(states_size = np.prod(env.observation_space.shape), \n",
    "                      actions_size   = env.action_space.n, \n",
    "                      hidden_size = 16,\n",
    "                      embedding_size = 32,\n",
    "                      hash_functions = 32, \n",
    "                      beta = 1)\n",
    "    \n",
    "    _, ride_states, ride_test_rewards, ride_ints, ride_percentages  = train_with_reward(env,\n",
    "                                                                        agent,\n",
    "                                                                        seed,\n",
    "                                                                        ride,\n",
    "                                                                        intrinsic_weight = 1,\n",
    "                                                                        n_episodes=time_num_episodes,\n",
    "                                                                        test_reward_period = test_reward_period,\n",
    "                                                                        update_reward_period = 100,\n",
    "                                                                        batch_size =  150,\n",
    "                                                                        n_iter =  50,\n",
    "                                                                        std_dev = 0.0,\n",
    "                                                                        annealing = True)\n",
    "    ride_all_agent_percentages.append(ride_percentages)\n",
    "    ride_all_states.append(ride_states)\n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}ride_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, ride_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}ride_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, ride_all_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bca375",
   "metadata": {},
   "source": [
    "# LBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalLinear(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, reparam_noise=1e-4):\n",
    "        super(VariationalLinear, self).__init__()\n",
    "        self.mu    = nn.Linear(num_inputs, num_outputs)\n",
    "        self.sigma = nn.Linear(num_inputs, num_outputs)\n",
    "        self.reparam_noise = reparam_noise\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.mu(x)\n",
    "        sigma = self.sigma(x)\n",
    "        sigma = F.softplus(sigma) + self.reparam_noise\n",
    "        return mu, sigma\n",
    "\n",
    "class LBS_Reward(BaseIntrinsicRewardModule):\n",
    "    def __init__(self,action_size, state_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.trans_det = nn.Sequential(\n",
    "                nn.Linear(state_size + action_size, hidden_size),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, hidden_size),                \n",
    "                nn.ReLU())\n",
    "        \n",
    "        self.trans_stoc = VariationalLinear(hidden_size, state_size)  \n",
    "        self.repr_model = nn.Sequential(\n",
    "                    nn.Linear(hidden_size + state_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                    VariationalLinear(hidden_size, state_size))\n",
    "        \n",
    "        self.beta = 1\n",
    "        self.head = nn.Linear(state_size, hidden_size) \n",
    "        \n",
    "        \n",
    "    def forward(self, state, action, next_state):\n",
    "        \n",
    "        # prior\n",
    "        s_a_combined = torch.cat([state, action], dim=-1).float()\n",
    "        trans_det_states = self.trans_det(s_a_combined)\n",
    "        \n",
    "        trans_stoch_mu, trans_stoch_sigma = self.trans_stoc(trans_det_states)\n",
    "        trans_stoch_distr = D.independent.Independent(D.Normal(trans_stoch_mu, trans_stoch_sigma), 1)\n",
    "\n",
    "        # posterior \n",
    "        ns_a_combined = torch.cat([trans_det_states, next_state], dim=-1) \n",
    "        \n",
    "        repr_stoch_mu, repr_stoch_sigma = self.repr_model(ns_a_combined)\n",
    "        repr_stoch_distr = D.independent.Independent(D.Normal(repr_stoch_mu, repr_stoch_sigma), 1)\n",
    "\n",
    "        return trans_det_states, trans_stoch_distr, repr_stoch_distr\n",
    "    \n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "\n",
    "        state = torch.flatten(state, start_dim=0) \n",
    "        action = torch.tensor(action).squeeze(0) \n",
    "        next_state = torch.flatten(next_state, start_dim=0) \n",
    "\n",
    "        _, trans_pred_distr, repr_pred_distr = self.forward(state, action, next_state)\n",
    "        \n",
    "        intrinsic_reward = D.kl.kl_divergence(repr_pred_distr, trans_pred_distr) # .mean(-1)\n",
    "        \n",
    "        return intrinsic_reward.detach().numpy()\n",
    "\n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch):        \n",
    "    \n",
    "        target_next_states, trans_pred_distr, repr_pred_distr = self.forward(state_batch, action_batch, next_state_batch)\n",
    "        repr_samples = repr_pred_distr.rsample()\n",
    "\n",
    "        target_distr = D.independent.Independent(D.Normal(target_next_states, torch.ones_like(target_next_states)), 1)\n",
    "        \n",
    "        repr_projections = self.head(repr_samples)\n",
    "        logprob_target = target_distr.log_prob(repr_projections)\n",
    "        \n",
    "        kl_div_post_prior = D.kl.kl_divergence(repr_pred_distr, trans_pred_distr) \n",
    "\n",
    "        loss = (self.beta * kl_div_post_prior - logprob_target).mean()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430fbfb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.distributions as D\n",
    "\n",
    "num_agents = 3\n",
    "lbs_all_agent_percentages = []\n",
    "lbs_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    time_num_episodes = 5000        \n",
    "    test_reward_period = 100\n",
    "    states_size = np.prod(env.observation_space.shape)\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = True)\n",
    "\n",
    "    lbs = LBS_Reward(action_size = 1, \n",
    "                     state_size  = np.prod(env.observation_space.shape),\n",
    "                     hidden_size = 16)\n",
    "    \n",
    "\n",
    "    lbs_rewards, lbs_states, lbs_test_rewards, lbs_ints, lbs_percentages = train_with_reward(env,\n",
    "                                                                        agent,\n",
    "                                                                        seed,\n",
    "                                                                        lbs,\n",
    "                                                                        intrinsic_weight = 1,\n",
    "                                                                        n_episodes=time_num_episodes,\n",
    "                                                                        test_reward_period = test_reward_period,\n",
    "                                                                        update_reward_period = 100,\n",
    "                                                                        batch_size =  150,\n",
    "                                                                        n_iter =  50,\n",
    "                                                                        std_dev = 0.0, \n",
    "                                                                        annealing = True)\n",
    "    lbs_all_agent_percentages.append(lbs_percentages)\n",
    "    lbs_all_states.append(lbs_states)\n",
    "    \n",
    "    \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}lbs_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, lbs_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}lbs_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, lbs_all_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abbb8a9",
   "metadata": {},
   "source": [
    "# Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44861024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, states_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.module = MLP(states_size,\n",
    "                          embedding_size,\n",
    "                          hidden_size)\n",
    "    def forward(self, s):\n",
    "        return self.module(s)\n",
    "\n",
    "class GaussianForwardDynamics(nn.Module):\n",
    "    def __init__(self, encoding_dim, action_size, latent_dim):\n",
    "        super().__init__()\n",
    "     \n",
    "        self.fc = nn.Linear(encoding_dim + action_size, encoding_dim)\n",
    "        self.fc_mu = nn.Linear(encoding_dim, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(encoding_dim, latent_dim)\n",
    "\n",
    "    def forward(self, latent_state, action):\n",
    "        \n",
    "        x  = torch.cat([latent_state, action], dim=-1)    \n",
    "        x  = F.gelu(self.fc(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_log_var(x)\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "class SurprisalModule(BaseIntrinsicRewardModule):\n",
    "    def __init__(self, input_size, action_size, linear_size, hidden_size, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedder = Embedder(input_size, \n",
    "                                 linear_size, \n",
    "                                 hidden_size)\n",
    "        \n",
    "        self.forward_model = GaussianForwardDynamics(hidden_size, \n",
    "                                                     1,\n",
    "                                                     hidden_size)\n",
    "        \n",
    "        self.n = 1\n",
    "        self.eta0 = 0\n",
    "\n",
    "    def get_loss_vec(self, state_batch, action_batch, next_state_batch):\n",
    "\n",
    "        phi_s_batch = self.embedder(state_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            phi_next_s_batch = self.embedder(next_state_batch)\n",
    "        \n",
    "        mu, log_var = self.forward_model.forward(phi_s_batch, action_batch)\n",
    "        dist = torch.distributions.MultivariateNormal(mu, torch.diag_embed(torch.exp(log_var)))\n",
    "        loss = -dist.log_prob(phi_next_s_batch)\n",
    "        return loss\n",
    "\n",
    "    def get_intrinsic_reward(self, state, action, next_state):\n",
    "        with torch.no_grad():\n",
    "            loss_vec = self.get_loss_vec(state, action, next_state)\n",
    "            intrinsic_reward = self.normalise_reward(loss_vec)\n",
    "        return intrinsic_reward[0] \n",
    "    \n",
    "    def get_loss(self, state_batch, action_batch, next_state_batch, *args, **kwargs):\n",
    "        loss_vec = self.get_loss_vec(state_batch, action_batch, next_state_batch)\n",
    "        loss = torch.mean(loss_vec)\n",
    "        return loss\n",
    "    \n",
    "    def normalise_reward(self, rewards_batch):\n",
    "        if rewards_batch.shape[0] == 1:\n",
    "            return rewards_batch\n",
    "        \n",
    "        mean_rewards = torch.abs(torch.mean(rewards_batch).view(rewards_batch.shape[0], 1))\n",
    "        norm_rewards = (rewards_batch - torch.min([0, mean_rewards])) / torch.max([1, mean_rewards.squeeze()] )\n",
    "        return norm_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d970537f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "surp_all_agent_percentages = []\n",
    "surp_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    time_num_episodes = 5000        \n",
    "    test_reward_period = 100\n",
    "    states_size = np.prod(env.observation_space.shape)\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = True)\n",
    "    \n",
    "    surprisal =  SurprisalModule(input_size = np.prod(env.observation_space.shape),  \n",
    "                                 action_size = env.action_space.n, \n",
    "                                 linear_size = 32, \n",
    "                                 hidden_size = 16)\n",
    "        \n",
    "    _, surp_states, surp_test_rewards, surp_ints, surp_percentages = train_with_reward(env,\n",
    "                                                                        agent,\n",
    "                                                                        seed,\n",
    "                                                                        surprisal,\n",
    "                                                                        intrinsic_weight = 1,\n",
    "                                                                        n_episodes=time_num_episodes,\n",
    "                                                                        test_reward_period = test_reward_period,\n",
    "                                                                        update_reward_period = 100,\n",
    "                                                                        batch_size =  150,\n",
    "                                                                        n_iter =  50,\n",
    "                                                                        std_dev = 0.0,\n",
    "                                                                        annealing = True)\n",
    "    surp_all_agent_percentages.append(surp_percentages)\n",
    "    surp_all_states.append(surp_states)\n",
    "        \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}surp_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, surp_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}surp_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, surp_all_states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a856bf",
   "metadata": {},
   "source": [
    "# LP Surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad05a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "class LPSurprisalModule(SurprisalModule):\n",
    "        def __init__(self, input_size, action_size, linear_size,hidden_size,learning_progress_n=1, *args,  **kwargs):\n",
    "            super().__init__(input_size, action_size, linear_size, hidden_size, *args, **kwargs)\n",
    "            self._old_forward_models = deque(maxlen = learning_progress_n)\n",
    "            self._old_embedders = deque(maxlen = learning_progress_n)\n",
    "\n",
    "        def get_loss_vec(self, state_batch, action_batch, next_state_batch, old=False):\n",
    "            # If there haaven't been enough old models return 0\n",
    "            if old:\n",
    "                embedder = self._old_embedders[0]\n",
    "                model = self._old_forward_models[0]\n",
    "            else:\n",
    "                embedder = self.embedder\n",
    "                model = self.forward_model\n",
    "            \n",
    "            phi_s_batch = embedder(state_batch)\n",
    "            with T.no_grad():\n",
    "                phi_next_s_batch = embedder(next_state_batch)\n",
    "\n",
    "            mu, log_var = model.forward(phi_s_batch, action_batch)\n",
    "            dist = torch.distributions.MultivariateNormal(mu, T.diag_embed(T.exp(log_var)))\n",
    "            loss_vec = -dist.log_prob(phi_next_s_batch)\n",
    "            \n",
    "            return loss_vec\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def update_old_models(self):\n",
    "            old_model = deepcopy(self.forward_model)\n",
    "            old_model.eval()\n",
    "\n",
    "            old_embedder = deepcopy(self.embedder)\n",
    "            old_embedder.eval()\n",
    "            self._old_embedders.append(old_embedder)\n",
    "            self._old_forward_models.append(old_model)\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def get_intrinsic_reward(self, state, action, next_state):\n",
    "            if len(self._old_forward_models) == 0:\n",
    "                intrinsic_reward = torch.zeros(state.shape[0],)\n",
    "                return intrinsic_reward.squeeze()\n",
    "            else:\n",
    "                old_loss = self.get_loss_vec(state, action, next_state, old=True)\n",
    "                new_loss = self.get_loss_vec(state, action, next_state, old=False)\n",
    "                intrinsic_reward = self.normalise_reward(new_loss - old_loss)\n",
    "                return intrinsic_reward.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a42df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "lp_surp_all_agent_percentages = []\n",
    "lp_surp_all_states = []\n",
    "\n",
    "for i in range(num_agents):\n",
    "    seed = i\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    env = gym.make(\"bsuite/deep_sea-v0\", size=24, seed=42)\n",
    "\n",
    "    epsilon = 0.1\n",
    "    time_num_episodes = 5000        \n",
    "    test_reward_period = 100\n",
    "    states_size = np.prod(env.observation_space.shape)\n",
    "    agent = QLearningAgent(alpha    = 0.5, \n",
    "                           epsilon  = epsilon,\n",
    "                           discount = 0.9,\n",
    "                           get_legal_actions=lambda s: range(env.action_space.n),\n",
    "                           item = True)\n",
    "\n",
    "    \n",
    "    \n",
    "    lp_surprisal =  LPSurprisalModule(input_size = np.prod(env.observation_space.shape),  \n",
    "                                   action_size = env.action_space.n, \n",
    "                                   linear_size = 32, \n",
    "                                   hidden_size = 16,\n",
    "                                   learning_progress_n=10)\n",
    "\n",
    "    _, lp_surp_states, lp_surp_test_rewards, lp_surp_ints, lp_surp_percentages = train_with_reward(env,\n",
    "                                                                        agent,\n",
    "                                                                        seed, \n",
    "                                                                        surprisal,\n",
    "                                                                        intrinsic_weight = 1,\n",
    "                                                                        n_episodes=time_num_episodes,\n",
    "                                                                        test_reward_period = test_reward_period,\n",
    "                                                                        update_reward_period = 100,\n",
    "                                                                        batch_size =  150,\n",
    "                                                                        n_iter =  50,\n",
    "                                                                        std_dev = 0.0,\n",
    "                                                                        annealing = True)\n",
    "    lp_surp_all_agent_percentages.append(lp_surp_percentages)\n",
    "    lp_surp_all_states.append(lp_surp_states)\n",
    "        \n",
    "save_dir = f'saved_deepsea_results/'\n",
    "precentages_path = f\"{save_dir}lp_surp_percentages_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(precentages_path, lp_surp_all_agent_percentages)\n",
    "\n",
    "save_dir = f'saved_deepsea_results/'\n",
    "state_visits_path = f\"{save_dir}lp_surp_states_{env.observation_space.shape[0]}.npy\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(state_visits_path, lp_surp_all_states[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
